{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain + Deep Lake + ChatGPT-3.5 快速学习github开源代码 \n",
    "\n",
    "#### 1. dependences\n",
    "##### 1.1 deeplake - online vectors DB\n",
    "##### 1.2 tiktoken - convert the text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.268)\n",
      "Requirement already satisfied: deeplake in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.19)\n",
      "Requirement already satisfied: openai in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.0)\n",
      ".........botocore<1.32.0,>=1.31.30->boto3->deeplake) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade langchain deeplake openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "os.environ.clear()\n",
    "load_dotenv()\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "token = os.environ['ACTIVELOOP_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt-engineer'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AntonOsika/gpt-engineer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = '../brian_jupyter_lab/gpt-engineer/'\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(os.path.join(dirpath, file),encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1540, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# 对文本切块，8000的字符切一次，每个切块的上下文件有100个字符的重叠\n",
    "text_splitter = CharacterTextSplitter(chunk_size=8000, chunk_overlap=100)\n",
    "texts= text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://huangliang/bjl_gpt_enginner_source already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://huangliang/bjl_gpt_enginner_source', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (425, 1536)  float32   None   \n",
      "    id        text      (425, 1)      str     None   \n",
      " metadata     json      (425, 1)      str     None   \n",
      "   text       text      (425, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['50bb68a7-3f30-11ee-b60b-dc215ccacab9',\n",
       " '50bb68a8-3f30-11ee-98a7-dc215ccacab9',\n",
       " '50bb68a9-3f30-11ee-b082-dc215ccacab9',\n",
       " '50bb68aa-3f30-11ee-b8f9-dc215ccacab9',\n",
       " '50bb68ab-3f30-11ee-9aea-dc215ccacab9',\n",
       " 。。。。。。。。,\n",
       " '50bca165-3f30-11ee-ba3d-dc215ccacab9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username=\"huangliang\"\n",
    "db = DeepLake(dataset_path=f\"hub://{username}/gpt_enginner_02\", embedding=embeddings,token=token)\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://huangliang/gpt_enginner_02 already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path='hub://huangliang/gpt_enginner_02', read_only=True,embedding=embeddings,token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()\n",
    "#distance_metric计算两个的点的距离\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "#fetch_k 从文档中要检索的数据点的数据\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "#k 返回相似度最高的向量的数量  \n",
    "retriever.search_kwargs['k'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\",openai_api_key=api_key,temperature=0.1)\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " what dose gpt-engineer work? \n",
      "\n",
      "A:\n",
      " GPT-Engineer is a project that uses OpenAI's GPT-4 model to automate various software engineering tasks. It provides a conversational interface to interact with the GPT-4 model and guide it to generate code, clarify instructions, generate specifications, and more. The project is designed to be easy to use, even for users without a background in coding. It leverages the capabilities of GPT-4 to automate software engineering processes and make development tasks more efficient. \n",
      "\n",
      "Q:\n",
      " How to use gpt-engineer? \n",
      "\n",
      "A:\n",
      " To use gpt-engineer, you can follow these general steps:\n",
      "\n",
      "1. Install gpt-engineer: You can install gpt-engineer by running `pip install gpt-engineer` or by cloning the GitHub repository and installing it from the source code.\n",
      "\n",
      "2. Set up your project: Create a new folder for your project and copy the example project files into it. Fill in the `prompt` file with your desired prompt or instructions.\n",
      "\n",
      "3. Set your OpenAI API key: Export your OpenAI API key as an environment variable by running `export OPENAI_API_KEY=[your api key]` in your terminal or command prompt.\n",
      "\n",
      "4. Run gpt-engineer: Execute the gpt-engineer command with the path to your project folder as the argument. For example, `gpt-engineer projects/my-new-project`.\n",
      "\n",
      "5. Review the results: Check the generated files in the `workspace` folder of your project. These files will contain the code or other outputs generated by gpt-engineer based on your prompt.\n",
      "\n",
      "Please note that these are general steps, and you may need to customize them based on your specific use case and requirements. It's also recommended to refer to the official documentation and examples provided by the gpt-engineer project for more detailed instructions and guidance. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"what dose gpt-engineer work?\",\n",
    "    \"How to use gpt-engineer?\"\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\":question,\"chat_history\":chat_history})\n",
    "    chat_history.append((question,result['answer']))\n",
    "    print(f\"Q:\\n {question} \\n\")\n",
    "    print(f\"A:\\n {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question,chat_history):\n",
    "    res = qa({\"question\":question,\"chat_history\":chat_history})\n",
    "    print(f\"Q:\\n {question} \\n\")\n",
    "    print(f\"A:\\n {res['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " What's the main programming language used in gpt-engineer? \n",
      "\n",
      "A:\n",
      " The primary programming language used in GPT-Engineer is Python. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"What's the main programming language used in gpt-engineer?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " What's python class to generate the prompt in gpt-engineer \n",
      "\n",
      "A:\n",
      " The Python class used to generate prompts in GPT-Engineer is the `AI` class. It serves as the main interface to the GPT-4 model and provides methods to start a conversation with the model, continue an existing conversation, and format system and user messages. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"What's python class to generate the prompt in gpt-engineer\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Can tell me the file in 'preprompts' floder of gpt-engineer? \n",
      "\n",
      "A:\n",
      " In the 'preprompts' folder of GPT-Engineer, you can find the following files:\n",
      "\n",
      "1. fix_code\n",
      "2. generate\n",
      "3. philosophy\n",
      "4. qa\n",
      "5. respec\n",
      "6. spec\n",
      "7. unit_tests\n",
      "8. use_feedback\n",
      "\n",
      "These files contain predefined prompts that guide the AI in performing different tasks. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Can tell me the file in 'preprompts' floder of gpt-engineer?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me the order of gpt-engineer use the prompt file in 'preprompts' floder? \n",
      "\n",
      "A:\n",
      " The order of using the prompt file in the 'preprompts' folder for GPT-Engineer is as follows:\n",
      "\n",
      "1. Fix Code (`gpt_engineer/preprompts/fix_code`)\n",
      "2. Generate (`gpt_engineer/preprompts/generate`)\n",
      "3. Philosophy (`gpt_engineer/preprompts/philosophy`)\n",
      "4. QA (`gpt_engineer/preprompts/qa`)\n",
      "5. Respec (`gpt_engineer/preprompts/respec`)\n",
      "6. Spec (`gpt_engineer/preprompts/spec`)\n",
      "7. Unit Tests (`gpt_engineer/preprompts/unit_tests`)\n",
      "8. Use Feedback (`gpt_engineer/preprompts/use_feedback`)\n",
      "\n",
      "This order represents a sequence of different tasks that the AI can perform based on the instructions provided in each prompt. It starts with fixing code, followed by generating code, considering best practices, seeking clarification, reviewing specifications, writing unit tests, and utilizing user feedback for code generation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me the order of gpt-engineer use the prompt file in 'preprompts' floder?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me the Python class of gpt-engineer use the prompt file in 'preprompts' floder? \n",
      "\n",
      "A:\n",
      " The Python class used by gpt-engineer to access the prompt file in the 'preprompts' folder is the `DB` class. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me the Python class of gpt-engineer use the prompt file in 'preprompts' floder?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me what dose DB.py do in gpt-engineer ? \n",
      "\n",
      "A:\n",
      " The purpose of `DB.py` in GPT-Engineer is to represent a simple database that stores its data as files in a directory. It serves as a key-value store, where keys are filenames and values are file contents. The `DB` class provides methods to check if a key (filename) exists in the database, get the value (file content) associated with a key, and set the value associated with a key. The `DB` class is responsible for managing the data used by the GPT-Engineer system. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me what dose DB.py do in gpt-engineer ?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me Where dose DB.py store the data in gpt-engineer ? \n",
      "\n",
      "A:\n",
      " The `DB.py` file in `gpt-engineer` represents a simple database that stores its data as files in a directory. The directory where the data is stored can be specified when creating an instance of the `DB` class by providing a path argument. If the directory does not already exist, the `DB` class will create it. Each file in the directory represents a key-value pair, where the filename serves as the key and the file content serves as the value. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me Where dose DB.py store the data in gpt-engineer ?\",chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
