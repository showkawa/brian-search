{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain + Deep Lake + ChatGPT-3.5 快速学习github开源代码 \n",
    "\n",
    "#### 1. dependences\n",
    "##### 1.1 deeplake - online vectors DB\n",
    "##### 1.2 tiktoken - convert the text to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.268)\n",
      "Requirement already satisfied: deeplake in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.19)\n",
      "Requirement already satisfied: openai in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\hh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.0)\n",
      ".........botocore<1.32.0,>=1.31.30->boto3->deeplake) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain deeplake openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-'\n",
    "os.environ['ACTIVELOOP_TOKEN'] = 'eyJhbGciOiJIUzUxMiIsImlhdCI6MTY5MjUxODU3OCwiZXhwIjoxNjkyNjA0OTU3fQ.eyJpZCI6Imh1YW5nbGlhbmcifQ.WJV8JwCrWmyPz7wuxgZdSt8IusvVMPr1MERyZzC4u4XegS7iMP-T3eMKaf2kz7rwuJ1CuYSW4Odws6vkz6nw9w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\" env --> {os.environ['OPENAI_API_KEY']}\")\n",
    "#print(f\" env --> {os.environ['DEEP_LAKE_API_KEY']}\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt-engineer'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AntonOsika/gpt-engineer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = '../brian_jupyter_lab/gpt-engineer/'\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(os.path.join(dirpath, file),encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1540, which is longer than the specified 1500\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# 对文本切块，2000的字符切一次，每个切块的上下文件有10个字符的重叠\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=10)\n",
    "texts= text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://huangliang/bjl_gpt_enginner_source already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://huangliang/bjl_gpt_enginner_source', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (425, 1536)  float32   None   \n",
      "    id        text      (425, 1)      str     None   \n",
      " metadata     json      (425, 1)      str     None   \n",
      "   text       text      (425, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['50bb68a7-3f30-11ee-b60b-dc215ccacab9',\n",
       " '50bb68a8-3f30-11ee-98a7-dc215ccacab9',\n",
       " '50bb68a9-3f30-11ee-b082-dc215ccacab9',\n",
       " '50bb68aa-3f30-11ee-b8f9-dc215ccacab9',\n",
       " '50bb68ab-3f30-11ee-9aea-dc215ccacab9',\n",
       " 。。。。。。。。,\n",
       " '50bca165-3f30-11ee-ba3d-dc215ccacab9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username=\"huangliang\"\n",
    "db = DeepLake(dataset_path=f\"hub://{username}/bjl_gpt_enginner_source\", embedding=embeddings)\n",
    "db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://huangliang/bjl_gpt_enginner_source already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path='hub://huangliang/bjl_gpt_enginner_source', read_only=True,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()\n",
    "#distance_metric计算两个的点的距离\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "#fetch_k 从文档中要检索的数据点的数据\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "#k 返回相似度最高的向量的数量  \n",
    "retriever.search_kwargs['k'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " what dose gpt-engineer work? \n",
      "\n",
      "A:\n",
      " GPT-Engineer is a project that leverages the capabilities of OpenAI's GPT-4 language model to automate various software engineering tasks. It provides a powerful tool for automating code generation, clarifying instructions, generating specifications, and more. The project is built with a modular architecture, making it easy to extend and customize for different use cases. The core components of GPT-Engineer include the AI class, the DB class, and the steps module, which work together to provide its functionality. Users can interact with the GPT-4 model in a conversational manner, providing prompts in plain English and guiding the model to generate the desired output. The generated code is saved as files in a workspace and can be executed independently of the GPT-Engineer system. \n",
      "\n",
      "Q:\n",
      " How to use gpt-engineer? \n",
      "\n",
      "A:\n",
      " To use GPT-Engineer, you can follow these steps:\n",
      "\n",
      "1. **Installation**: Install the GPT-Engineer package and its dependencies.\n",
      "\n",
      "2. **Setting Up Your Project**: Set up a workspace for your project, including the necessary directories and files.\n",
      "\n",
      "3. **Running GPT-Engineer**: Start GPT-Engineer and initiate a conversation with the AI model. You can provide prompts in plain English to guide the model's code generation.\n",
      "\n",
      "4. **Reviewing the Results**: After each interaction with the AI, review the generated code and messages. You can save the generated code as files in your workspace.\n",
      "\n",
      "It's important to note that GPT-Engineer is an experimental tool, and it's always a good idea to review and validate the generated code for correctness and adherence to best practices. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"what dose gpt-engineer work?\",\n",
    "    \"How to use gpt-engineer?\"\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\":question,\"chat_history\":chat_history})\n",
    "    chat_history.append((question,result['answer']))\n",
    "    print(f\"Q:\\n {question} \\n\")\n",
    "    print(f\"A:\\n {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question,chat_history):\n",
    "    res = qa({\"question\":question,\"chat_history\":chat_history})\n",
    "    print(f\"Q:\\n {question} \\n\")\n",
    "    print(f\"A:\\n {res['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " What's the main programming language used in gpt-engineer? \n",
      "\n",
      "A:\n",
      " The primary programming language used in GPT-Engineer is Python. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"What's the main programming language used in gpt-engineer?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " What's python class to generate the prompt in gpt-engineer \n",
      "\n",
      "A:\n",
      " The Python class used to generate prompts in GPT-Engineer is the `AI` class. It serves as the main interface to the GPT-4 model and provides methods to start a conversation with the model, continue an existing conversation, and format system and user messages. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"What's python class to generate the prompt in gpt-engineer\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Can tell me the file in 'preprompts' floder of gpt-engineer? \n",
      "\n",
      "A:\n",
      " In the 'preprompts' folder of GPT-Engineer, you can find the following files:\n",
      "\n",
      "1. fix_code\n",
      "2. generate\n",
      "3. philosophy\n",
      "4. qa\n",
      "5. respec\n",
      "6. spec\n",
      "7. unit_tests\n",
      "8. use_feedback\n",
      "\n",
      "These files contain predefined prompts that guide the AI in performing different tasks. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Can tell me the file in 'preprompts' floder of gpt-engineer?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me the order of gpt-engineer use the prompt file in 'preprompts' floder? \n",
      "\n",
      "A:\n",
      " The order of using the prompt file in the 'preprompts' folder for GPT-Engineer is as follows:\n",
      "\n",
      "1. Fix Code (`gpt_engineer/preprompts/fix_code`)\n",
      "2. Generate (`gpt_engineer/preprompts/generate`)\n",
      "3. Philosophy (`gpt_engineer/preprompts/philosophy`)\n",
      "4. QA (`gpt_engineer/preprompts/qa`)\n",
      "5. Respec (`gpt_engineer/preprompts/respec`)\n",
      "6. Spec (`gpt_engineer/preprompts/spec`)\n",
      "7. Unit Tests (`gpt_engineer/preprompts/unit_tests`)\n",
      "8. Use Feedback (`gpt_engineer/preprompts/use_feedback`)\n",
      "\n",
      "This order represents a sequence of different tasks that the AI can perform based on the instructions provided in each prompt. It starts with fixing code, followed by generating code, considering best practices, seeking clarification, reviewing specifications, writing unit tests, and utilizing user feedback for code generation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me the order of gpt-engineer use the prompt file in 'preprompts' floder?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me the Python class of gpt-engineer use the prompt file in 'preprompts' floder? \n",
      "\n",
      "A:\n",
      " The Python class used by gpt-engineer to access the prompt file in the 'preprompts' folder is the `DB` class. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me the Python class of gpt-engineer use the prompt file in 'preprompts' floder?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me what dose DB.py do in gpt-engineer ? \n",
      "\n",
      "A:\n",
      " The purpose of `DB.py` in GPT-Engineer is to represent a simple database that stores its data as files in a directory. It serves as a key-value store, where keys are filenames and values are file contents. The `DB` class provides methods to check if a key (filename) exists in the database, get the value (file content) associated with a key, and set the value associated with a key. The `DB` class is responsible for managing the data used by the GPT-Engineer system. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me what dose DB.py do in gpt-engineer ?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me Where dose DB.py store the data in gpt-engineer ? \n",
      "\n",
      "A:\n",
      " The `DB.py` file in `gpt-engineer` represents a simple database that stores its data as files in a directory. The directory where the data is stored can be specified when creating an instance of the `DB` class by providing a path argument. If the directory does not already exist, the `DB` class will create it. Each file in the directory represents a key-value pair, where the filename serves as the key and the file content serves as the value. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me Where dose DB.py store the data in gpt-engineer ?\",chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
