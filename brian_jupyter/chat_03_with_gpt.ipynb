{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chat with GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.21) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "os.environ.clear()\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,openai_api_key=os.environ.get('OPENAI_API_KEY'), model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Here it is by default set to \"AI\"\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(conversation\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39mtemplate)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conversation' is not defined"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "def track_tokens_usage(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f\"prompt token {cb.prompt_tokens}, res token {cb.completion_tokens}, total token {cb.total_tokens}\")\n",
    "        print(f\"successful requests {cb.successful_requests}, total cost {cb.total_cost}USD\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Java 中tuple的使用？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "prompt token 69, res token 152, total token 221\n",
      "successful requests 1, total cost 0.00040750000000000004USD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'在Java中，没有内置的tuple类型。然而，你可以使用其他方式来实现类似的功能。一种常见的方法是使用数组或集合来存储多个值。另一种方法是创建一个自定义的类来表示一个tuple，该类可以包含多个字段来存储不同的值。你还可以使用第三方库，如Apache Commons或Google Guava，它们提供了tuple类型的实现。这些库通常提供了不同长度的tuple，例如Pair、Triple等。你可以根据自己的需求选择合适的库来使用tuple。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_tokens_usage(conversation, \"Java 中tuple的使用？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用提示词模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are the author for auto-gpt, when other developer ask your question for auto-gpt, you should reply with chinese.', additional_kwargs={}),\n",
       " HumanMessage(content=\"what's the architecture for auto-gpt?\", additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template= \"You are the author for auto-gpt, when other developer ask your question for auto-gpt, you should reply with chinese.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "system_template = \"{user_input}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "chat_prompt.format_prompt(\n",
    "    user_input= \"what's the architecture for auto-gpt?\",\n",
    "\n",
    ").to_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
