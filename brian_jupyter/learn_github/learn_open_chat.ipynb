{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No broken requirements found.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip check openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# os.environ.clear()\n",
    "# RuntimeError: Could not determine home directory.\n",
    "load_dotenv()\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "token = os.environ['ACTIVELOOP_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/KillianLucas/open-interpreter.git ./../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "repo_path = \"../../../open-interpreter\"\n",
    "\n",
    "# load the repository\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path+\"/interpreter\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the docs into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=3000, chunk_overlap=50)\n",
    "texts= text_splitter.split_documents(documents)\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " page_content='\"\"\"\\nRight off the bat, to any contributors (a message from Killian):\\n\\nFirst of all, THANK YOU. Open Interpreter is ALIVE, ALL OVER THE WORLD because of YOU.\\n\\nWhile this project is rapidly growing, I\\'ve decided it\\'s best for us to allow some technical debt.\\n\\nThe code here has duplication. It has imports in weird places. It has been spaghettified to add features more quickly.\\n\\nIn my opinion **this is critical** to keep up with the pace of demand for this project.\\n\\nAt the same time, I plan on pushing a significant re-factor of `interpreter.py` and `code_interpreter.py` ~ September 11th.\\n\\nAfter the re-factor, Open Interpreter\\'s source code will be much simpler, and much more fun to dive into.\\n\\nEspecially if you have ideas and **EXCITEMENT** about the future of this project, chat with me on discord: https://discord.gg/6p3fD6rBVm\\n\\n- killian\\n\"\"\"\\n\\nimport argparse\\nimport os\\nfrom dotenv import load_dotenv\\nimport requests\\nfrom packaging import version\\nimport pkg_resources\\nfrom rich import print as rprint\\nfrom rich.markdown import Markdown\\nimport inquirer\\n\\n# Load .env file\\nload_dotenv()\\n\\ndef check_for_update():\\n    # Fetch the latest version from the PyPI API\\n    response = requests.get(f\\'https://pypi.org/pypi/open-interpreter/json\\')\\n    latest_version = response.json()[\\'info\\'][\\'version\\']\\n\\n    # Get the current version using pkg_resources\\n    current_version = pkg_resources.get_distribution(\"open-interpreter\").version\\n\\n    return version.parse(latest_version) > version.parse(current_version)' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\cli.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='def cli(interpreter):\\n  \"\"\"\\n  Takes an instance of interpreter.\\n  Modifies it according to command line flags, then runs chat.\\n  \"\"\"\\n\\n  try:\\n    if check_for_update():\\n      print(\"A new version is available. Please run \\'pip install --upgrade open-interpreter\\'.\")\\n  except:\\n    # Fine if this fails\\n    pass\\n\\n  # Load values from .env file with the new names\\n  AUTO_RUN = os.getenv(\\'INTERPRETER_CLI_AUTO_RUN\\', \\'False\\') == \\'True\\'\\n  FAST_MODE = os.getenv(\\'INTERPRETER_CLI_FAST_MODE\\', \\'False\\') == \\'True\\'\\n  LOCAL_RUN = os.getenv(\\'INTERPRETER_CLI_LOCAL_RUN\\', \\'False\\') == \\'True\\'\\n  DEBUG = os.getenv(\\'INTERPRETER_CLI_DEBUG\\', \\'False\\') == \\'True\\'\\n  USE_AZURE = os.getenv(\\'INTERPRETER_CLI_USE_AZURE\\', \\'False\\') == \\'True\\'\\n\\n  # Setup CLI\\n  parser = argparse.ArgumentParser(description=\\'Chat with Open Interpreter.\\')\\n  \\n  parser.add_argument(\\'-y\\',\\n                      \\'--yes\\',\\n                      action=\\'store_true\\',\\n                      default=AUTO_RUN,\\n                      help=\\'execute code without user confirmation\\')\\n  parser.add_argument(\\'-f\\',\\n                      \\'--fast\\',\\n                      action=\\'store_true\\',\\n                      default=FAST_MODE,\\n                      help=\\'use gpt-3.5-turbo instead of gpt-4\\')\\n  parser.add_argument(\\'-l\\',\\n                      \\'--local\\',\\n                      action=\\'store_true\\',\\n                      default=LOCAL_RUN,\\n                      help=\\'run fully local with code-llama\\')\\n  parser.add_argument(\\n                      \\'--falcon\\',\\n                      action=\\'store_true\\',\\n                      default=False,\\n                      help=\\'run fully local with falcon-40b\\')\\n  parser.add_argument(\\'-d\\',\\n                      \\'--debug\\',\\n                      action=\\'store_true\\',\\n                      default=DEBUG,\\n                      help=\\'prints extra information\\')\\n  \\n  parser.add_argument(\\'--model\\',\\n                      type=str,\\n                      help=\\'model name (for OpenAI compatible APIs) or HuggingFace repo\\',\\n                      default=\"\",\\n                      required=False)\\n  \\n  parser.add_argument(\\'--max_tokens\\',\\n                      type=int,\\n                      help=\\'max tokens generated (for locally run models)\\')\\n  parser.add_argument(\\'--context_window\\',\\n                      type=int,\\n                      help=\\'context window in tokens (for locally run models)\\')\\n  \\n  parser.add_argument(\\'--api_base\\',\\n                      type=str,\\n                      help=\\'change your api_base to any OpenAI compatible api\\',\\n                      default=\"\",\\n                      required=False)\\n  \\n  parser.add_argument(\\'--use-azure\\',\\n                      action=\\'store_true\\',\\n                      default=USE_AZURE,\\n                      help=\\'use Azure OpenAI Services\\')\\n  \\n  parser.add_argument(\\'--version\\',\\n                      action=\\'store_true\\',\\n                      help=\\'display current Open Interpreter version\\')\\n\\n  args = parser.parse_args()' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\cli.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='args = parser.parse_args()\\n\\n\\n  if args.version:\\n    print(\"Open Interpreter\", pkg_resources.get_distribution(\"open-interpreter\").version)\\n    return\\n\\n  if args.max_tokens:\\n    interpreter.max_tokens = args.max_tokens\\n  if args.context_window:\\n    interpreter.context_window = args.context_window\\n\\n  # Modify interpreter according to command line flags\\n  if args.yes:\\n    interpreter.auto_run = True\\n  if args.fast:\\n    interpreter.model = \"gpt-3.5-turbo\"\\n  if args.local and not args.falcon:\\n\\n\\n\\n    # Temporarily, for backwards (behavioral) compatability, we\\'ve moved this part of llama_2.py here.\\n    # This way, when folks hit interpreter --local, they get the same experience as before.\\n    \\n    rprint(\\'\\', Markdown(\"**Open Interpreter** will use `Code Llama` for local execution. Use your arrow keys to set up the model.\"), \\'\\')\\n        \\n    models = {\\n        \\'7B\\': \\'TheBloke/CodeLlama-7B-Instruct-GGUF\\',\\n        \\'13B\\': \\'TheBloke/CodeLlama-13B-Instruct-GGUF\\',\\n        \\'34B\\': \\'TheBloke/CodeLlama-34B-Instruct-GGUF\\'\\n    }\\n    \\n    parameter_choices = list(models.keys())\\n    questions = [inquirer.List(\\'param\\', message=\"Parameter count (smaller is faster, larger is more capable)\", choices=parameter_choices)]\\n    answers = inquirer.prompt(questions)\\n    chosen_param = answers[\\'param\\']\\n\\n    # THIS is more in line with the future. You just say the model you want by name:\\n    interpreter.model = models[chosen_param]\\n    interpreter.local = True\\n\\n  \\n  if args.debug:\\n    interpreter.debug_mode = True\\n  if args.use_azure:\\n    interpreter.use_azure = True\\n    interpreter.local = False\\n\\n\\n  if args.model != \"\":\\n    interpreter.model = args.model\\n\\n    # \"/\" in there means it\\'s a HF repo we\\'re going to run locally:\\n    if \"/\" in interpreter.model:\\n      interpreter.local = True\\n\\n  if args.api_base:\\n    interpreter.api_base = args.api_base\\n\\n  if args.falcon or args.model == \"tiiuae/falcon-180B\": # because i tweeted <-this by accident lol, we actually need TheBloke\\'s quantized version of Falcon:\\n\\n    # Temporarily, for backwards (behavioral) compatability, we\\'ve moved this part of llama_2.py here.\\n    # This way, when folks hit interpreter --falcon, they get the same experience as --local.\\n    \\n    rprint(\\'\\', Markdown(\"**Open Interpreter** will use `Falcon` for local execution. Use your arrow keys to set up the model.\"), \\'\\')\\n        \\n    models = {\\n        \\'7B\\': \\'TheBloke/CodeLlama-7B-Instruct-GGUF\\',\\n        \\'40B\\': \\'YokaiKoibito/falcon-40b-GGUF\\',\\n        \\'180B\\': \\'TheBloke/Falcon-180B-Chat-GGUF\\'\\n    }\\n    \\n    parameter_choices = list(models.keys())\\n    questions = [inquirer.List(\\'param\\', message=\"Parameter count (smaller is faster, larger is more capable)\", choices=parameter_choices)]\\n    answers = inquirer.prompt(questions)\\n    chosen_param = answers[\\'param\\']\\n\\n    if chosen_param == \"180B\":\\n      rprint(Markdown(\"> **WARNING:** To run `Falcon-180B` we recommend at least `100GB` of RAM.\"))' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\cli.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# THIS is more in line with the future. You just say the model you want by name:\\n    interpreter.model = models[chosen_param]\\n    interpreter.local = True\\n\\n\\n  # Run the chat method\\n  interpreter.chat()' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\cli.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='from rich.live import Live\\nfrom rich.panel import Panel\\nfrom rich.box import MINIMAL\\nfrom rich.syntax import Syntax\\nfrom rich.table import Table\\nfrom rich.console import Group\\nfrom rich.console import Console\\n\\n\\nclass CodeBlock:\\n  \"\"\"\\n  Code Blocks display code and outputs in different languages.\\n  \"\"\"\\n\\n  def __init__(self):\\n    # Define these for IDE auto-completion\\n    self.language = \"\"\\n    self.output = \"\"\\n    self.code = \"\"\\n    self.active_line = None\\n\\n    self.live = Live(auto_refresh=False, console=Console(), vertical_overflow=\"visible\")\\n    self.live.start()\\n\\n  def update_from_message(self, message):\\n    if \"function_call\" in message and \"parsed_arguments\" in message[\\n        \"function_call\"]:\\n\\n      parsed_arguments = message[\"function_call\"][\"parsed_arguments\"]\\n\\n      if parsed_arguments != None:\\n        self.language = parsed_arguments.get(\"language\")\\n        self.code = parsed_arguments.get(\"code\")\\n\\n        if self.code and self.language:\\n          self.refresh()\\n\\n  def end(self):\\n    self.refresh(cursor=False)\\n    # Destroys live display\\n    self.live.stop()\\n\\n  def refresh(self, cursor=True):\\n    # Get code, return if there is none\\n    code = self.code\\n    if not code:\\n      return\\n    \\n    # Create a table for the code\\n    code_table = Table(show_header=False,\\n                       show_footer=False,\\n                       box=None,\\n                       padding=0,\\n                       expand=True)\\n    code_table.add_column()\\n\\n    # Add cursor    \\n    if cursor:\\n      code += \"█\"\\n\\n    # Add each line of code to the table\\n    code_lines = code.strip().split(\\'\\\\n\\')\\n    for i, line in enumerate(code_lines, start=1):\\n      if i == self.active_line:\\n        # This is the active line, print it with a white background\\n        syntax = Syntax(line, self.language, theme=\"bw\", line_numbers=False, word_wrap=True)\\n        code_table.add_row(syntax, style=\"black on white\")\\n      else:\\n        # This is not the active line, print it normally\\n        syntax = Syntax(line, self.language, theme=\"monokai\", line_numbers=False, word_wrap=True)\\n        code_table.add_row(syntax)\\n\\n    # Create a panel for the code\\n    code_panel = Panel(code_table, box=MINIMAL, style=\"on #272722\")\\n\\n    # Create a panel for the output (if there is any)\\n    if self.output == \"\" or self.output == \"None\":\\n      output_panel = \"\"\\n    else:\\n      output_panel = Panel(self.output,\\n                           box=MINIMAL,\\n                           style=\"#FFFFFF on #3b3b37\")\\n\\n    # Create a group with the code table and output panel\\n    group = Group(\\n      code_panel,\\n      output_panel,\\n    )\\n\\n    # Update the live display\\n    self.live.update(group)\\n    self.live.refresh()' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_block.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='\"\"\"\\nRight off the bat, to any contributors (a message from Killian):\\n\\nFirst of all, THANK YOU. Open Interpreter is ALIVE, ALL OVER THE WORLD because of YOU.\\n\\nWhile this project is rapidly growing, I\\'ve decided it\\'s best for us to allow some technical debt.\\n\\nThe code here has duplication. It has imports in weird places. It has been spaghettified to add features more quickly.\\n\\nIn my opinion **this is critical** to keep up with the pace of demand for this project.\\n\\nAt the same time, I plan on pushing a significant re-factor of `interpreter.py` and `code_interpreter.py` ~ September 11th.\\n\\nAfter the re-factor, Open Interpreter\\'s source code will be much simpler, and much more fun to dive into.\\n\\nEspecially if you have ideas and **EXCITEMENT** about the future of this project, chat with me on discord: https://discord.gg/6p3fD6rBVm\\n\\n- killian\\n\"\"\"\\n\\nimport subprocess\\nimport webbrowser\\nimport tempfile\\nimport threading\\nimport traceback\\nimport platform\\nimport time\\nimport ast\\nimport sys\\nimport os\\nimport re\\n\\n\\ndef run_html(html_content):\\n    # Create a temporary HTML file with the content\\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".html\") as f:\\n        f.write(html_content.encode())\\n\\n    # Open the HTML file with the default web browser\\n    webbrowser.open(\\'file://\\' + os.path.realpath(f.name))\\n\\n    return f\"Saved to {os.path.realpath(f.name)} and opened with the user\\'s default web browser.\"\\n\\n\\n# Mapping of languages to their start, run, and print commands\\nlanguage_map = {\\n  \"python\": {\\n    # Python is run from this interpreter with sys.executable\\n    # in interactive, quiet, and unbuffered mode\\n    \"start_cmd\": sys.executable + \" -i -q -u\",\\n    \"print_cmd\": \\'print(\"{}\")\\'\\n  },\\n  \"R\": {\\n    # R is run from this interpreter with R executable\\n    # in interactive, quiet, and unbuffered mode\\n    \"start_cmd\": \"R -q --vanilla\",\\n    \"print_cmd\": \\'print(\"{}\")\\'\\n  },\\n  \"shell\": {\\n    # On Windows, the shell start command is `cmd.exe`\\n    # On Unix, it should be the SHELL environment variable (defaults to \\'bash\\' if not set)\\n    \"start_cmd\": \\'cmd.exe\\' if platform.system() == \\'Windows\\' else os.environ.get(\\'SHELL\\', \\'bash\\'),\\n    \"print_cmd\": \\'echo \"{}\"\\'\\n  },\\n  \"javascript\": {\\n    \"start_cmd\": \"node -i\",\\n    \"print_cmd\": \\'console.log(\"{}\")\\'\\n  },\\n  \"applescript\": {\\n    # Starts from shell, whatever the user\\'s preference (defaults to \\'/bin/zsh\\')\\n    # (We\\'ll prepend \"osascript -e\" every time, not once at the start, so we want an empty shell)\\n    \"start_cmd\": os.environ.get(\\'SHELL\\', \\'/bin/zsh\\'),\\n    \"print_cmd\": \\'log \"{}\"\\'\\n  },\\n  \"html\": {\\n    \"open_subrocess\": False,\\n    \"run_function\": run_html,\\n  }\\n}\\n\\n# Get forbidden_commands (disabled)\\n\"\"\"\\nwith open(\"interpreter/forbidden_commands.json\", \"r\") as f:\\n  forbidden_commands = json.load(f)\\n\"\"\"' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='class CodeInterpreter:\\n  \"\"\"\\n  Code Interpreters display and run code in different languages.\\n\\n  They can control code blocks on the terminal, then be executed to produce an output which will be displayed in real-time.\\n  \"\"\"\\n\\n  def __init__(self, language, debug_mode):\\n    self.language = language\\n    self.proc = None\\n    self.active_line = None\\n    self.debug_mode = debug_mode\\n\\n  def start_process(self):\\n    # Get the start_cmd for the selected language\\n    start_cmd = language_map[self.language][\"start_cmd\"]\\n\\n    # Use the appropriate start_cmd to execute the code\\n    self.proc = subprocess.Popen(start_cmd.split(),\\n                                 stdin=subprocess.PIPE,\\n                                 stdout=subprocess.PIPE,\\n                                 stderr=subprocess.PIPE,\\n                                 text=True,\\n                                 bufsize=0)\\n\\n    # Start watching ^ its `stdout` and `stderr` streams\\n    threading.Thread(target=self.save_and_display_stream,\\n                     args=(self.proc.stdout, False), # Passes False to is_error_stream\\n                     daemon=True).start()\\n    threading.Thread(target=self.save_and_display_stream,\\n                     args=(self.proc.stderr, True), # Passes True to is_error_stream\\n                     daemon=True).start()\\n\\n  def update_active_block(self):\\n      \"\"\"\\n      This will also truncate the output,\\n      which we need to do every time we update the active block.\\n      \"\"\"\\n      # Strip then truncate the output if necessary\\n      self.output = truncate_output(self.output)\\n\\n      # Display it\\n      self.active_block.active_line = self.active_line\\n      self.active_block.output = self.output\\n      self.active_block.refresh()\\n\\n  def run(self):\\n    \"\"\"\\n    Executes code.\\n    \"\"\"\\n\\n    # Get code to execute\\n    self.code = self.active_block.code\\n\\n    # Check for forbidden commands (disabled)\\n    \"\"\"\\n    for line in self.code.split(\"\\\\n\"):\\n      if line in forbidden_commands:\\n        message = f\"This code contains a forbidden command: {line}\"\\n        message += \"\\\\n\\\\nPlease contact the Open Interpreter team if this is an error.\"\\n        self.active_block.output = message\\n        return message\\n    \"\"\"\\n\\n    # Should we keep a subprocess open? True by default\\n    open_subrocess = language_map[self.language].get(\"open_subrocess\", True)\\n\\n    # Start the subprocess if it hasn\\'t been started\\n    if not self.proc and open_subrocess:\\n      try:\\n        self.start_process()\\n      except:\\n        # Sometimes start_process will fail!\\n        # Like if they don\\'t have `node` installed or something.\\n\\n        traceback_string = traceback.format_exc()\\n        self.output = traceback_string\\n        self.update_active_block()\\n\\n        # Before you return, wait for the display to catch up?\\n        # (I\\'m not sure why this works)\\n        time.sleep(0.1)\\n\\n        return self.output\\n\\n    # Reset output\\n    self.output = \"\"' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Reset output\\n    self.output = \"\"\\n\\n    # Use the print_cmd for the selected language\\n    self.print_cmd = language_map[self.language].get(\"print_cmd\")\\n    code = self.code\\n\\n    # Add print commands that tell us what the active line is\\n    if self.print_cmd:\\n      try:\\n        code = self.add_active_line_prints(code)\\n      except:\\n        # If this failed, it means the code didn\\'t compile\\n        # This traceback will be our output.\\n\\n        traceback_string = traceback.format_exc()\\n        self.output = traceback_string\\n        self.update_active_block()\\n\\n        # Before you return, wait for the display to catch up?\\n        # (I\\'m not sure why this works)\\n        time.sleep(0.1)\\n\\n        return self.output\\n\\n    if self.language == \"python\":\\n      # This lets us stop execution when error happens (which is not default -i behavior)\\n      # And solves a bunch of indentation problems-- if everything\\'s indented, -i treats it as one block\\n      code = wrap_in_try_except(code)\\n\\n    # Remove any whitespace lines, as this will break indented blocks\\n    # (are we sure about this? test this)\\n    code_lines = code.split(\"\\\\n\")\\n    code_lines = [c for c in code_lines if c.strip() != \"\"]\\n    code = \"\\\\n\".join(code_lines)\\n\\n    # Add end command (we\\'ll be listening for this so we know when it ends)\\n    if self.print_cmd and self.language != \"applescript\": # Applescript is special. Needs it to be a shell command because \\'return\\' (very common) will actually return, halt script\\n      code += \"\\\\n\\\\n\" + self.print_cmd.format(\\'END_OF_EXECUTION\\')\\n\\n    # Applescript-specific processing\\n    if self.language == \"applescript\":\\n      # Escape double quotes\\n      code = code.replace(\\'\"\\', r\\'\\\\\"\\')\\n      # Wrap in double quotes\\n      code = \\'\"\\' + code + \\'\"\\'\\n      # Prepend start command\\n      code = \"osascript -e \" + code\\n      # Append end command\\n      code += \\'\\\\necho \"END_OF_EXECUTION\"\\'\\n\\n    # Debug\\n    if self.debug_mode:\\n      print(\"Running code:\")\\n      print(code)\\n      print(\"---\")\\n\\n    # HTML-specific processing (and running)\\n    if self.language == \"html\":\\n      output = language_map[\"html\"][\"run_function\"](code)\\n      return output\\n\\n    # Reset self.done so we can .wait() for it\\n    self.done = threading.Event()\\n    self.done.clear()\\n\\n    # Write code to stdin of the process\\n    try:\\n      self.proc.stdin.write(code + \"\\\\n\")\\n      self.proc.stdin.flush()\\n    except BrokenPipeError:\\n      # It can just.. break sometimes? Let\\'s fix this better in the future\\n      # For now, just try again\\n      self.start_process()\\n      self.run()\\n      return\\n\\n    # Wait until execution completes\\n    self.done.wait()\\n\\n    # Before you return, wait for the display to catch up?\\n    # (I\\'m not sure why this works)\\n    time.sleep(0.1)\\n\\n    # Return code output\\n    return self.output' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Return code output\\n    return self.output\\n\\n  def add_active_line_prints(self, code):\\n    \"\"\"\\n    This function takes a code snippet and adds print statements before each line,\\n    indicating the active line number during execution. The print statements respect\\n    the indentation of the original code, using the indentation of the next non-blank line.\\n\\n    Note: This doesn\\'t work on shell if:\\n    1) Any line starts with whitespace and\\n    2) Sometimes, doesn\\'t even work for regular loops with newlines between lines\\n    We return in those cases.\\n    3) It really struggles with multiline stuff, so I\\'ve disabled that (but we really should fix and restore).\\n    \"\"\"\\n\\n    if self.language == \"python\":\\n      return add_active_line_prints_to_python(code)\\n\\n    # Split the original code into lines\\n    code_lines = code.strip().split(\\'\\\\n\\')\\n\\n    # If it\\'s shell, check for breaking cases\\n    if self.language == \"shell\":\\n      if len(code_lines) > 1:\\n        return code\\n      if \"for\" in code or \"do\" in code or \"done\" in code:\\n        return code\\n      for line in code_lines:\\n        if line.startswith(\" \"):\\n          return code\\n\\n    # Initialize an empty list to hold the modified lines of code\\n    modified_code_lines = []\\n\\n    # Iterate over each line in the original code\\n    for i, line in enumerate(code_lines):\\n      # Initialize a variable to hold the leading whitespace of the next non-empty line\\n      leading_whitespace = \"\"\\n\\n      # Iterate over the remaining lines to find the leading whitespace of the next non-empty line\\n      for next_line in code_lines[i:]:\\n        if next_line.strip():\\n          leading_whitespace = next_line[:len(next_line) -\\n                                         len(next_line.lstrip())]\\n          break\\n\\n      # Format the print command with the current line number, using the found leading whitespace\\n      print_line = self.print_cmd.format(f\"ACTIVE_LINE:{i+1}\")\\n      print_line = leading_whitespace + print_line\\n\\n      # Add the print command and the original line to the modified lines\\n      modified_code_lines.append(print_line)\\n      modified_code_lines.append(line)\\n\\n    # Join the modified lines with newlines and return the result\\n    code = \"\\\\n\".join(modified_code_lines)\\n    return code\\n\\n  def save_and_display_stream(self, stream, is_error_stream):\\n    # Handle each line of output\\n    for line in iter(stream.readline, \\'\\'):\\n\\n      if self.debug_mode:\\n        print(\"Recieved output line:\")\\n        print(line)\\n        print(\"---\")\\n\\n      line = line.strip()\\n\\n      # Node\\'s interactive REPL outputs a billion things\\n      # So we clean it up:\\n      if self.language == \"javascript\":\\n        if \"Welcome to Node.js\" in line:\\n          continue\\n        if line in [\"undefined\", \\'Type \".help\" for more information.\\']:\\n          continue\\n        # Remove trailing \">\"s\\n        line = re.sub(r\\'^\\\\s*(>\\\\s*)+\\', \\'\\', line)' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Python\\'s interactive REPL outputs a million things\\n      # So we clean it up:\\n      if self.language == \"python\":\\n        if re.match(r\\'^(\\\\s*>>>\\\\s*|\\\\s*\\\\.\\\\.\\\\.\\\\s*)\\', line):\\n          continue\\n\\n      # R\\'s interactive REPL outputs a million things\\n      # So we clean it up:\\n      if self.language == \"R\":\\n        if re.match(r\\'^(\\\\s*>>>\\\\s*|\\\\s*\\\\.\\\\.\\\\.\\\\s*)\\', line):\\n          continue\\n\\n      # Check if it\\'s a message we added (like ACTIVE_LINE)\\n      # Or if we should save it to self.output\\n      if line.startswith(\"ACTIVE_LINE:\"):\\n        self.active_line = int(line.split(\":\")[1])\\n      elif \"END_OF_EXECUTION\" in line:\\n        self.done.set()\\n        self.active_line = None\\n      elif is_error_stream and \"KeyboardInterrupt\" in line:\\n        raise KeyboardInterrupt\\n      else:\\n        self.output += \"\\\\n\" + line\\n        self.output = self.output.strip()\\n\\n      self.update_active_block()' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='def truncate_output(data):\\n  needs_truncation = False\\n\\n  # In the future, this will come from a config file\\n  max_output_chars = 2000\\n\\n  message = f\\'Output truncated. Showing the last {max_output_chars} characters.\\\\n\\\\n\\'\\n\\n  # Remove previous truncation message if it exists\\n  if data.startswith(message):\\n    data = data[len(message):]\\n    needs_truncation = True\\n\\n  # If data exceeds max length, truncate it and add message\\n  if len(data) > max_output_chars or needs_truncation:\\n    data = message + data[-max_output_chars:]\\n\\n  return data\\n\\n# Perhaps we should split the \"add active line prints\" processing to a new file?\\n# Add active prints to python:' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='class AddLinePrints(ast.NodeTransformer):\\n    \"\"\"\\n    Transformer to insert print statements indicating the line number\\n    before every executable line in the AST.\\n    \"\"\"\\n\\n    def insert_print_statement(self, line_number):\\n        \"\"\"Inserts a print statement for a given line number.\"\"\"\\n        return ast.Expr(\\n            value=ast.Call(\\n                func=ast.Name(id=\\'print\\', ctx=ast.Load()),\\n                args=[ast.Constant(value=f\"ACTIVE_LINE:{line_number}\")],\\n                keywords=[]\\n            )\\n        )\\n\\n    def process_body(self, body):\\n        \"\"\"Processes a block of statements, adding print calls.\"\"\"\\n        new_body = []\\n\\n        # In case it\\'s not iterable:\\n        if not isinstance(body, list):\\n            body = [body]\\n\\n        for sub_node in body:\\n            if hasattr(sub_node, \\'lineno\\'):\\n                new_body.append(self.insert_print_statement(sub_node.lineno))\\n            new_body.append(sub_node)\\n\\n        return new_body\\n\\n    def visit(self, node):\\n        \"\"\"Overridden visit to transform nodes.\"\"\"\\n        new_node = super().visit(node)\\n\\n        # If node has a body, process it\\n        if hasattr(new_node, \\'body\\'):\\n            new_node.body = self.process_body(new_node.body)\\n\\n        # If node has an orelse block (like in for, while, if), process it\\n        if hasattr(new_node, \\'orelse\\') and new_node.orelse:\\n            new_node.orelse = self.process_body(new_node.orelse)\\n\\n        # Special case for Try nodes as they have multiple blocks\\n        if isinstance(new_node, ast.Try):\\n            for handler in new_node.handlers:\\n                handler.body = self.process_body(handler.body)\\n            if new_node.finalbody:\\n                new_node.finalbody = self.process_body(new_node.finalbody)\\n\\n        return new_node\\n\\ndef add_active_line_prints_to_python(code):\\n    \"\"\"\\n    Add print statements indicating line numbers to a python string.\\n    \"\"\"\\n    tree = ast.parse(code)\\n    transformer = AddLinePrints()\\n    new_tree = transformer.visit(tree)\\n    return ast.unparse(new_tree)' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='def wrap_in_try_except(code):\\n    # Add import traceback\\n    code = \"import traceback\\\\n\" + code\\n\\n    # Parse the input code into an AST\\n    parsed_code = ast.parse(code)\\n\\n    # Wrap the entire code\\'s AST in a single try-except block\\n    try_except = ast.Try(\\n        body=parsed_code.body,\\n        handlers=[\\n            ast.ExceptHandler(\\n                type=ast.Name(id=\"Exception\", ctx=ast.Load()),\\n                name=None,\\n                body=[\\n                    ast.Expr(\\n                        value=ast.Call(\\n                            func=ast.Attribute(value=ast.Name(id=\"traceback\", ctx=ast.Load()), attr=\"print_exc\", ctx=ast.Load()),\\n                            args=[],\\n                            keywords=[]\\n                        )\\n                    ),\\n                ]\\n            )\\n        ],\\n        orelse=[],\\n        finalbody=[]\\n    )\\n\\n    # Assign the try-except block as the new body\\n    parsed_code.body = [try_except]\\n\\n    # Convert the modified AST back to source code\\n    return ast.unparse(parsed_code)' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\code_interpreter.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='\"\"\"\\nRight off the bat, to any contributors (a message from Killian):\\n\\nFirst of all, THANK YOU. Open Interpreter is ALIVE, ALL OVER THE WORLD because of YOU.\\n\\nWhile this project is rapidly growing, I\\'ve decided it\\'s best for us to allow some technical debt.\\n\\nThe code here has duplication. It has imports in weird places. It has been spaghettified to add features more quickly.\\n\\nIn my opinion **this is critical** to keep up with the pace of demand for this project.\\n\\nAt the same time, I plan on pushing a significant re-factor of `interpreter.py` and `code_interpreter.py` ~ September 11th.\\n\\nAfter the re-factor, Open Interpreter\\'s source code will be much simpler, and much more fun to dive into.\\n\\nEspecially if you have ideas and **EXCITEMENT** about the future of this project, chat with me on discord: https://discord.gg/6p3fD6rBVm\\n\\n- killian\\n\"\"\"\\n\\nimport os\\nimport sys\\nimport appdirs\\nimport traceback\\nimport inquirer\\nimport subprocess\\nfrom rich import print\\nfrom rich.markdown import Markdown\\nimport os\\nimport inquirer\\nfrom huggingface_hub import list_files_info, hf_hub_download' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\get_hf_llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='def get_hf_llm(repo_id, debug_mode, context_window):\\n\\n    if \"TheBloke/CodeLlama-\" not in repo_id:\\n      # ^ This means it was prob through the old --local, so we have already displayed this message.\\n      # Hacky. Not happy with this\\n      print(\\'\\', Markdown(f\"**Open Interpreter** will use `{repo_id}` for local execution. Use your arrow keys to set up the model.\"), \\'\\')\\n\\n    raw_models = list_gguf_files(repo_id)\\n    \\n    if not raw_models:\\n        print(f\"Failed. Are you sure there are GGUF files in `{repo_id}`?\")\\n        return None\\n\\n    combined_models = group_and_combine_splits(raw_models)\\n\\n    selected_model = None\\n\\n    # First we give them a simple small medium large option. If they want to see more, they can.\\n\\n    if len(combined_models) > 3:\\n\\n        # Display Small Medium Large options to user\\n        choices = [\\n            format_quality_choice(combined_models[0], \"Small\"),\\n            format_quality_choice(combined_models[len(combined_models) // 2], \"Medium\"),\\n            format_quality_choice(combined_models[-1], \"Large\"),\\n            \"See More\"\\n        ]\\n        questions = [inquirer.List(\\'selected_model\\', message=\"Quality (smaller is faster, larger is more capable)\", choices=choices)]\\n        answers = inquirer.prompt(questions)\\n        if answers[\"selected_model\"].startswith(\"Small\"):\\n            selected_model = combined_models[0][\"filename\"]\\n        elif answers[\"selected_model\"].startswith(\"Medium\"):\\n            selected_model = combined_models[len(combined_models) // 2][\"filename\"]\\n        elif answers[\"selected_model\"].startswith(\"Large\"):\\n            selected_model = combined_models[-1][\"filename\"]\\n    \\n    if selected_model == None:\\n        # This means they either selected See More,\\n        # Or the model only had 1 or 2 options\\n\\n        # Display to user\\n        choices = [format_quality_choice(model) for model in combined_models]\\n        questions = [inquirer.List(\\'selected_model\\', message=\"Quality (smaller is faster, larger is more capable)\", choices=choices)]\\n        answers = inquirer.prompt(questions)\\n        for model in combined_models:\\n            if format_quality_choice(model) == answers[\"selected_model\"]:\\n                selected_model = model[\"filename\"]\\n                break\\n\\n    # Third stage: GPU confirm\\n    if confirm_action(\"Use GPU? (Large models might crash on GPU, but will run more quickly)\"):\\n      n_gpu_layers = -1\\n    else:\\n      n_gpu_layers = 0\\n\\n    # Get user data directory\\n    user_data_dir = appdirs.user_data_dir(\"Open Interpreter\")\\n    default_path = os.path.join(user_data_dir, \"models\")\\n\\n    # Ensure the directory exists\\n    os.makedirs(default_path, exist_ok=True)\\n\\n    # Define the directories to check\\n    directories_to_check = [\\n        default_path,\\n        \"llama.cpp/models/\",\\n        os.path.expanduser(\"~\") + \"/llama.cpp/models/\",\\n        \"/\"\\n    ]' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\get_hf_llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Check for the file in each directory\\n    for directory in directories_to_check:\\n        path = os.path.join(directory, selected_model)\\n        if os.path.exists(path):\\n            model_path = path\\n            break\\n    else:\\n        # If the file was not found, ask for confirmation to download it\\n        download_path = os.path.join(default_path, selected_model)\\n      \\n        print(f\"This language model was not found on your system.\\\\n\\\\nDownload to `{default_path}`?\", \"\")\\n        if confirm_action(\"\"):\\n          \\n            # Check if model was originally split\\n            split_files = [model[\"filename\"] for model in raw_models if selected_model in model[\"filename\"]]\\n            \\n            if len(split_files) > 1:\\n                # Download splits\\n                for split_file in split_files:\\n                    hf_hub_download(repo_id=repo_id, filename=split_file, local_dir=default_path, local_dir_use_symlinks=False)\\n                \\n                # Combine and delete splits\\n                actually_combine_files(selected_model, split_files)\\n            else:\\n                hf_hub_download(repo_id=repo_id, filename=selected_model, local_dir=default_path, local_dir_use_symlinks=False)\\n\\n            model_path = download_path\\n        \\n        else:\\n            print(\\'\\\\n\\', \"Download cancelled. Exiting.\", \\'\\\\n\\')\\n            return None\\n\\n    # This is helpful for folks looking to delete corrupted ones and such\\n    print(Markdown(f\"Model found at `{model_path}`\"))\\n  \\n    try:\\n        from llama_cpp import Llama\\n    except:\\n        if debug_mode:\\n            traceback.print_exc()\\n        # Ask for confirmation to install the required pip package\\n        message = \"Local LLM interface package not found. Install `llama-cpp-python`?\"\\n        if confirm_action(message):\\n            \\n            # We\\'re going to build llama-cpp-python correctly for the system we\\'re on' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\get_hf_llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='import platform\\n            \\n            def check_command(command):\\n                try:\\n                    subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n                    return True\\n                except subprocess.CalledProcessError:\\n                    return False\\n                except FileNotFoundError:\\n                    return False\\n            \\n            def install_llama(backend):\\n                env_vars = {\\n                    \"FORCE_CMAKE\": \"1\"\\n                }\\n                \\n                if backend == \"cuBLAS\":\\n                    env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\\n                elif backend == \"hipBLAS\":\\n                    env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_HIPBLAS=on\"\\n                elif backend == \"Metal\":\\n                    env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_METAL=on\"\\n                else:  # Default to OpenBLAS\\n                    env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"\\n                \\n                try:\\n                    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"llama-cpp-python\"], env=env_vars, check=True)\\n                except subprocess.CalledProcessError as e:\\n                    print(f\"Error during installation with {backend}: {e}\")\\n            \\n            def supports_metal():\\n                # Check for macOS version\\n                if platform.system() == \"Darwin\":\\n                    mac_version = tuple(map(int, platform.mac_ver()[0].split(\\'.\\')))\\n                    # Metal requires macOS 10.11 or later\\n                    if mac_version >= (10, 11):\\n                        return True\\n                return False\\n            \\n            # Check system capabilities\\n            if check_command([\"nvidia-smi\"]):\\n                install_llama(\"cuBLAS\")\\n            elif check_command([\"rocminfo\"]):\\n                install_llama(\"hipBLAS\")\\n            elif supports_metal():\\n                install_llama(\"Metal\")\\n            else:\\n                install_llama(\"OpenBLAS\")\\n          \\n            from llama_cpp import Llama\\n            print(\\'\\', Markdown(\"Finished downloading `Code-Llama` interface.\"), \\'\\')\\n\\n            # Tell them if their architecture won\\'t work well' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\get_hf_llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Check if on macOS\\n            if platform.system() == \"Darwin\":\\n                # Check if it\\'s Apple Silicon\\n                if platform.machine() != \"arm64\":\\n                    print(\"Warning: You are using Apple Silicon (M1/M2) Mac but your Python is not of \\'arm64\\' architecture.\")\\n                    print(\"The llama.ccp x86 version will be 10x slower on Apple Silicon (M1/M2) Mac.\")\\n                    print(\"\\\\nTo install the correct version of Python that supports \\'arm64\\' architecture:\")\\n                    print(\"1. Download Miniforge for M1/M2:\")\\n                    print(\"wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\")\\n                    print(\"2. Install it:\")\\n                    print(\"bash Miniforge3-MacOSX-arm64.sh\")\\n                    print(\"\")\\n      \\n        else:\\n            print(\\'\\', \"Installation cancelled. Exiting.\", \\'\\')\\n            return None\\n\\n    # Initialize and return Code-Llama\\n    assert os.path.isfile(model_path)\\n    llama_2 = Llama(model_path=model_path, n_gpu_layers=n_gpu_layers, verbose=debug_mode, n_ctx=context_window)\\n      \\n    return llama_2' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\get_hf_llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='def confirm_action(message):\\n    question = [\\n        inquirer.Confirm(\\'confirm\\',\\n                         message=message,\\n                         default=True),\\n    ]\\n\\n    answers = inquirer.prompt(question)\\n    return answers[\\'confirm\\']\\n\\n\\n\\n\\n\\n\\nimport os\\nimport inquirer\\nfrom huggingface_hub import list_files_info, hf_hub_download, login\\nfrom typing import Dict, List, Union\\n\\ndef list_gguf_files(repo_id: str) -> List[Dict[str, Union[str, float]]]:\\n    \"\"\"\\n    Fetch all files from a given repository on Hugging Face Model Hub that contain \\'gguf\\'.\\n\\n    :param repo_id: Repository ID on Hugging Face Model Hub.\\n    :return: A list of dictionaries, each dictionary containing filename, size, and RAM usage of a model.\\n    \"\"\"\\n\\n    try:\\n      files_info = list_files_info(repo_id=repo_id)\\n    except Exception as e:\\n      if \"authentication\" in str(e).lower():\\n        print(\"You likely need to be logged in to HuggingFace to access this language model.\")\\n        print(f\"Visit this URL to log in and apply for access to this language model: https://huggingface.co/{repo_id}\")\\n        print(\"Then, log in here:\")\\n        login()\\n        files_info = list_files_info(repo_id=repo_id)\\n  \\n    gguf_files = [file for file in files_info if \"gguf\" in file.rfilename]\\n\\n    gguf_files = sorted(gguf_files, key=lambda x: x.size)\\n\\n    # Prepare the result\\n    result = []\\n    for file in gguf_files:\\n        size_in_gb = file.size / (1024**3)\\n        filename = file.rfilename\\n        result.append({\\n            \"filename\": filename,\\n            \"Size\": size_in_gb,\\n            \"RAM\": size_in_gb + 2.5,\\n        })\\n\\n    return result\\n\\nfrom typing import List, Dict, Union\\n\\ndef group_and_combine_splits(models: List[Dict[str, Union[str, float]]]) -> List[Dict[str, Union[str, float]]]:\\n    \"\"\"\\n    Groups filenames based on their base names and combines the sizes and RAM requirements.\\n\\n    :param models: List of model details.\\n    :return: A list of combined model details.\\n    \"\"\"\\n    grouped_files = {}\\n\\n    for model in models:\\n        base_name = model[\"filename\"].split(\\'-split-\\')[0]\\n        \\n        if base_name in grouped_files:\\n            grouped_files[base_name][\"Size\"] += model[\"Size\"]\\n            grouped_files[base_name][\"RAM\"] += model[\"RAM\"]\\n            grouped_files[base_name][\"SPLITS\"].append(model[\"filename\"])\\n        else:\\n            grouped_files[base_name] = {\\n                \"filename\": base_name,\\n                \"Size\": model[\"Size\"],\\n                \"RAM\": model[\"RAM\"],\\n                \"SPLITS\": [model[\"filename\"]]\\n            }\\n\\n    return list(grouped_files.values())' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\get_hf_llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='def actually_combine_files(base_name: str, files: List[str]) -> None:\\n    \"\"\"\\n    Combines files together and deletes the original split files.\\n\\n    :param base_name: The base name for the combined file.\\n    :param files: List of files to be combined.\\n    \"\"\"\\n    files.sort()    \\n    with open(base_name, \\'wb\\') as outfile:\\n        for file in files:\\n            with open(file, \\'rb\\') as infile:\\n                outfile.write(infile.read())\\n            os.remove(file)\\n\\ndef format_quality_choice(model, name_override = None) -> str:\\n    \"\"\"\\n    Formats the model choice for display in the inquirer prompt.\\n    \"\"\"\\n    if name_override:\\n        name = name_override\\n    else:\\n        name = model[\\'filename\\']\\n    return f\"{name} | Size: {model[\\'Size\\']:.1f} GB, Estimated RAM usage: {model[\\'RAM\\']:.1f} GB\"' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\get_hf_llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='class Interpreter:\\n\\n  def __init__(self):\\n    self.messages = []\\n    self.temperature = 0.001\\n    self.api_key = None\\n    self.auto_run = False\\n    self.local = False\\n    self.model = \"gpt-4\"\\n    self.debug_mode = False\\n    self.api_base = None # Will set it to whatever OpenAI wants\\n    self.context_window = 2000 # For local models only\\n    self.max_tokens = 750 # For local models only\\n    # Azure OpenAI\\n    self.use_azure = False\\n    self.azure_api_base = None\\n    self.azure_api_version = None\\n    self.azure_deployment_name = None\\n    self.azure_api_type = \"azure\"\\n\\n    # Get default system message\\n    here = os.path.abspath(os.path.dirname(__file__))\\n    with open(os.path.join(here, \\'system_message.txt\\'), \\'r\\') as f:\\n      self.system_message = f.read().strip()\\n\\n    # Store Code Interpreter instances for each language\\n    self.code_interpreters = {}\\n\\n    # No active block to start\\n    # (blocks are visual representation of messages on the terminal)\\n    self.active_block = None\\n\\n    # Note: While Open Interpreter can use Llama, we will prioritize gpt-4.\\n    # gpt-4 is faster, smarter, can call functions, and is all-around easier to use.\\n    # This makes gpt-4 better aligned with Open Interpreters priority to be easy to use.\\n    self.llama_instance = None\\n\\n  def cli(self):\\n    # The cli takes the current instance of Interpreter,\\n    # modifies it according to command line flags, then runs chat.\\n    cli(self)\\n\\n  def get_info_for_system_message(self):\\n    \"\"\"\\n    Gets relevent information for the system message.\\n    \"\"\"\\n\\n    info = \"\"\\n\\n    # Add user info\\n    username = getpass.getuser()\\n    current_working_directory = os.getcwd()\\n    operating_system = platform.system()\\n\\n    info += f\"[User Info]\\\\nName: {username}\\\\nCWD: {current_working_directory}\\\\nOS: {operating_system}\"\\n\\n    if not self.local:\\n\\n      # Open Procedures is an open-source database of tiny, structured coding tutorials.\\n      # We can query it semantically and append relevant tutorials/procedures to our system message:\\n\\n      # Use the last two messages\\' content or function call to semantically search\\n      query = []\\n      for message in self.messages[-2:]:\\n        message_for_semantic_search = {\"role\": message[\"role\"]}\\n        if \"content\" in message:\\n          message_for_semantic_search[\"content\"] = message[\"content\"]\\n        if \"function_call\" in message and \"parsed_arguments\" in message[\"function_call\"]:\\n          message_for_semantic_search[\"function_call\"] = message[\"function_call\"][\"parsed_arguments\"]\\n        query.append(message_for_semantic_search)\\n\\n      # Use them to query Open Procedures\\n      url = \"https://open-procedures.replit.app/search/\"' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='try:\\n        relevant_procedures = requests.get(url, data=json.dumps(query)).json()[\"procedures\"]\\n        info += \"\\\\n\\\\n# Recommended Procedures\\\\n\" + \"\\\\n---\\\\n\".join(relevant_procedures) + \"\\\\nIn your plan, include steps and, if present, **EXACT CODE SNIPPETS** (especially for depracation notices, **WRITE THEM INTO YOUR PLAN -- underneath each numbered step** as they will VANISH once you execute your first line of code, so WRITE THEM DOWN NOW if you need them) from the above procedures if they are relevant to the task. Again, include **VERBATIM CODE SNIPPETS** from the procedures above if they are relevent to the task **directly in your plan.**\"\\n      except:\\n        # For someone, this failed for a super secure SSL reason.\\n        # Since it\\'s not stricly necessary, let\\'s worry about that another day. Should probably log this somehow though.\\n        pass\\n\\n    elif self.local:\\n\\n      # Tell Code-Llama how to run code.\\n      info += \"\\\\n\\\\nTo run code, write a fenced code block (i.e ```python, R or ```shell) in markdown. When you close it with ```, it will be run. You\\'ll then be given its output.\"\\n      # We make references in system_message.txt to the \"function\" it can call, \"run_code\".\\n\\n    return info\\n\\n  def reset(self):\\n    self.messages = []\\n    self.code_interpreters = {}\\n\\n  def load(self, messages):\\n    self.messages = messages\\n\\n  def chat(self, message=None, return_messages=False):\\n\\n    # Connect to an LLM (an large language model)\\n    if not self.local:\\n      # gpt-4\\n      self.verify_api_key()\\n\\n    # ^ verify_api_key may set self.local to True, so we run this as an \\'if\\', not \\'elif\\':\\n    if self.local:\\n\\n      # Code-Llama\\n      if self.llama_instance == None:\\n\\n        # Find or install Code-Llama\\n        try:\\n          self.llama_instance = get_hf_llm(self.model, self.debug_mode, self.context_window)\\n          if self.llama_instance == None:\\n            # They cancelled.\\n            return\\n        except:\\n          traceback.print_exc()\\n          # If it didn\\'t work, apologize and switch to GPT-4\\n\\n          print(Markdown(\"\".join([\\n            f\"> Failed to install `{self.model}`.\",\\n            f\"\\\\n\\\\n**Common Fixes:** You can follow our simple setup docs at the link below to resolve common errors.\\\\n\\\\n```\\\\nhttps://github.com/KillianLucas/open-interpreter/tree/main/docs\\\\n```\",\\n            f\"\\\\n\\\\n**If you\\'ve tried that and you\\'re still getting an error, we have likely not built the proper `{self.model}` support for your system.**\",\\n            \"\\\\n\\\\n*( Running language models locally is a difficult task!* If you have insight into the best way to implement this across platforms/architectures, please join the Open Interpreter community Discord and consider contributing the project\\'s development. )\",\\n            \"\\\\n\\\\nPress enter to switch to `GPT-4` (recommended).\"\\n          ])))\\n          input()\\n\\n          # Switch to GPT-4\\n          self.local = False\\n          self.model = \"gpt-4\"\\n          self.verify_api_key()' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Display welcome message\\n    welcome_message = \"\"\\n\\n    if self.debug_mode:\\n      welcome_message += \"> Entered debug mode\"\\n\\n    # If self.local, we actually don\\'t use self.model\\n    # (self.auto_run is like advanced usage, we display no messages)\\n    if not self.local and not self.auto_run:\\n      welcome_message += f\"\\\\n> Model set to `{self.model.upper()}`\"\\n\\n    if self.local:\\n      welcome_message += f\"\\\\n> Model set to `{self.model}`\"\\n\\n    # If not auto_run, tell the user we\\'ll ask permission to run code\\n    # We also tell them here how to exit Open Interpreter\\n    if not self.auto_run:\\n      welcome_message += \"\\\\n\\\\n\" + confirm_mode_message\\n\\n    welcome_message = welcome_message.strip()\\n\\n    # Print welcome message with newlines on either side (aesthetic choice)\\n    # unless we\\'re starting with a blockquote (aesthetic choice)\\n    if welcome_message != \"\":\\n      if welcome_message.startswith(\">\"):\\n        print(Markdown(welcome_message), \\'\\')\\n      else:\\n        print(\\'\\', Markdown(welcome_message), \\'\\')\\n\\n    # Check if `message` was passed in by user\\n    if message:\\n      # If it was, we respond non-interactivley\\n      self.messages.append({\"role\": \"user\", \"content\": message})\\n      self.respond()\\n\\n    else:\\n      # If it wasn\\'t, we start an interactive chat\\n      while True:\\n        try:\\n          user_input = input(\"> \").strip()\\n        except EOFError:\\n          break\\n        except KeyboardInterrupt:\\n          print()  # Aesthetic choice\\n          break\\n\\n        # Use `readline` to let users up-arrow to previous user messages,\\n        # which is a common behavior in terminals.\\n        readline.add_history(user_input)\\n\\n        # Add the user message to self.messages\\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\\n\\n        # Let the user turn on debug mode mid-chat\\n        if user_input == \"%debug\":\\n            print(\\'\\', Markdown(\"> Entered debug mode\"), \\'\\')\\n            print(self.messages)\\n            self.debug_mode = True\\n            continue\\n\\n        # Respond, but gracefully handle CTRL-C / KeyboardInterrupt\\n        try:\\n          self.respond()\\n        except KeyboardInterrupt:\\n          pass\\n        finally:\\n          # Always end the active block. Multiple Live displays = issues\\n          self.end_active_block()\\n\\n    if return_messages:\\n        return self.messages' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='def verify_api_key(self):\\n    \"\"\"\\n    Makes sure we have an AZURE_API_KEY or OPENAI_API_KEY.\\n    \"\"\"\\n    if self.use_azure:\\n      all_env_available = (\\n        (\\'AZURE_API_KEY\\' in os.environ or \\'OPENAI_API_KEY\\' in os.environ) and\\n        \\'AZURE_API_BASE\\' in os.environ and\\n        \\'AZURE_API_VERSION\\' in os.environ and\\n        \\'AZURE_DEPLOYMENT_NAME\\' in os.environ)\\n      if all_env_available:\\n        self.api_key = os.environ.get(\\'AZURE_API_KEY\\') or os.environ[\\'OPENAI_API_KEY\\']\\n        self.azure_api_base = os.environ[\\'AZURE_API_BASE\\']\\n        self.azure_api_version = os.environ[\\'AZURE_API_VERSION\\']\\n        self.azure_deployment_name = os.environ[\\'AZURE_DEPLOYMENT_NAME\\']\\n        self.azure_api_type = os.environ.get(\\'AZURE_API_TYPE\\', \\'azure\\')\\n      else:\\n        # This is probably their first time here!\\n        print(\\'\\', Markdown(\"**Welcome to Open Interpreter.**\"), \\'\\')\\n        time.sleep(1)\\n\\n        print(Rule(style=\"white\"))\\n\\n        print(Markdown(missing_azure_info_message), \\'\\', Rule(style=\"white\"), \\'\\')\\n        response = input(\"Azure OpenAI API key: \")\\n\\n        if response == \"\":\\n          # User pressed `enter`, requesting Code-Llama\\n\\n          print(Markdown(\\n            \"> Switching to `Code-Llama`...\\\\n\\\\n**Tip:** Run `interpreter --local` to automatically use `Code-Llama`.\"),\\n                \\'\\')\\n          time.sleep(2)\\n          print(Rule(style=\"white\"))\\n\\n\\n\\n          # Temporarily, for backwards (behavioral) compatability, we\\'ve moved this part of llama_2.py here.\\n          # AND BELOW.\\n          # This way, when folks hit interpreter --local, they get the same experience as before.\\n          import inquirer\\n\\n          print(\\'\\', Markdown(\"**Open Interpreter** will use `Code Llama` for local execution. Use your arrow keys to set up the model.\"), \\'\\')\\n\\n          models = {\\n              \\'7B\\': \\'TheBloke/CodeLlama-7B-Instruct-GGUF\\',\\n              \\'13B\\': \\'TheBloke/CodeLlama-13B-Instruct-GGUF\\',\\n              \\'34B\\': \\'TheBloke/CodeLlama-34B-Instruct-GGUF\\'\\n          }\\n\\n          parameter_choices = list(models.keys())\\n          questions = [inquirer.List(\\'param\\', message=\"Parameter count (smaller is faster, larger is more capable)\", choices=parameter_choices)]\\n          answers = inquirer.prompt(questions)\\n          chosen_param = answers[\\'param\\']\\n\\n          # THIS is more in line with the future. You just say the model you want by name:\\n          self.model = models[chosen_param]\\n          self.local = True\\n\\n\\n\\n\\n          return' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='return\\n\\n        else:\\n          self.api_key = response\\n          self.azure_api_base = input(\"Azure OpenAI API base: \")\\n          self.azure_deployment_name = input(\"Azure OpenAI deployment name of GPT: \")\\n          self.azure_api_version = input(\"Azure OpenAI API version: \")\\n          print(\\'\\', Markdown(\\n            \"**Tip:** To save this key for later, run `export AZURE_API_KEY=your_api_key AZURE_API_BASE=your_api_base AZURE_API_VERSION=your_api_version AZURE_DEPLOYMENT_NAME=your_gpt_deployment_name` on Mac/Linux or `setx AZURE_API_KEY your_api_key AZURE_API_BASE your_api_base AZURE_API_VERSION your_api_version AZURE_DEPLOYMENT_NAME your_gpt_deployment_name` on Windows.\"),\\n                \\'\\')\\n          time.sleep(2)\\n          print(Rule(style=\"white\"))\\n\\n      litellm.api_type = self.azure_api_type\\n      litellm.api_base = self.azure_api_base\\n      litellm.api_version = self.azure_api_version\\n      litellm.api_key = self.api_key\\n    else:\\n      if self.api_key == None:\\n        if \\'OPENAI_API_KEY\\' in os.environ:\\n          self.api_key = os.environ[\\'OPENAI_API_KEY\\']\\n        else:\\n          # This is probably their first time here!\\n          print(\\'\\', Markdown(\"**Welcome to Open Interpreter.**\"), \\'\\')\\n          time.sleep(1)\\n\\n          print(Rule(style=\"white\"))\\n\\n          print(Markdown(missing_api_key_message), \\'\\', Rule(style=\"white\"), \\'\\')\\n          response = input(\"OpenAI API key: \")\\n\\n          if response == \"\":\\n              # User pressed `enter`, requesting Code-Llama\\n\\n              print(Markdown(\\n                \"> Switching to `Code-Llama`...\\\\n\\\\n**Tip:** Run `interpreter --local` to automatically use `Code-Llama`.\"),\\n                    \\'\\')\\n              time.sleep(2)\\n              print(Rule(style=\"white\"))\\n\\n\\n\\n              # Temporarily, for backwards (behavioral) compatability, we\\'ve moved this part of llama_2.py here.\\n              # AND ABOVE.\\n              # This way, when folks hit interpreter --local, they get the same experience as before.\\n              import inquirer\\n\\n              print(\\'\\', Markdown(\"**Open Interpreter** will use `Code Llama` for local execution. Use your arrow keys to set up the model.\"), \\'\\')\\n\\n              models = {\\n                  \\'7B\\': \\'TheBloke/CodeLlama-7B-Instruct-GGUF\\',\\n                  \\'13B\\': \\'TheBloke/CodeLlama-13B-Instruct-GGUF\\',\\n                  \\'34B\\': \\'TheBloke/CodeLlama-34B-Instruct-GGUF\\'\\n              }\\n\\n              parameter_choices = list(models.keys())\\n              questions = [inquirer.List(\\'param\\', message=\"Parameter count (smaller is faster, larger is more capable)\", choices=parameter_choices)]\\n              answers = inquirer.prompt(questions)\\n              chosen_param = answers[\\'param\\']\\n\\n              # THIS is more in line with the future. You just say the model you want by name:\\n              self.model = models[chosen_param]\\n              self.local = True\\n\\n\\n\\n\\n              return' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='return\\n\\n          else:\\n              self.api_key = response\\n              print(\\'\\', Markdown(\"**Tip:** To save this key for later, run `export OPENAI_API_KEY=your_api_key` on Mac/Linux or `setx OPENAI_API_KEY your_api_key` on Windows.\"), \\'\\')\\n              time.sleep(2)\\n              print(Rule(style=\"white\"))\\n\\n      litellm.api_key = self.api_key\\n      if self.api_base:\\n        litellm.api_base = self.api_base\\n\\n  def end_active_block(self):\\n    if self.active_block:\\n      self.active_block.end()\\n      self.active_block = None\\n\\n  def respond(self):\\n    # Add relevant info to system_message\\n    # (e.g. current working directory, username, os, etc.)\\n    info = self.get_info_for_system_message()\\n\\n    # This is hacky, as we should have a different (minified) prompt for CodeLLama,\\n    # but for now, to make the prompt shorter and remove \"run_code\" references, just get the first 2 lines:\\n    if self.local:\\n      self.system_message = \"\\\\n\".join(self.system_message.split(\"\\\\n\")[:2])\\n      self.system_message += \"\\\\nOnly do what the user asks you to do, then ask what they\\'d like to do next.\"\\n\\n    system_message = self.system_message + \"\\\\n\\\\n\" + info\\n\\n    if self.local:\\n      messages = tt.trim(self.messages, max_tokens=(self.context_window-self.max_tokens-25), system_message=system_message)\\n    else:\\n      messages = tt.trim(self.messages, self.model, system_message=system_message)\\n\\n    if self.debug_mode:\\n      print(\"\\\\n\", \"Sending `messages` to LLM:\", \"\\\\n\")\\n      print(messages)\\n      print()\\n\\n    # Make LLM call\\n    if not self.local:\\n      # GPT\\n      \\n      error = \"\"\\n      \\n      for _ in range(3):  # 3 retries\\n        try:\\n\\n            if self.use_azure:\\n              response = litellm.completion(\\n                  f\"azure/{self.azure_deployment_name}\",\\n                  messages=messages,\\n                  functions=[function_schema],\\n                  temperature=self.temperature,\\n                  stream=True,\\n                  )\\n            else:\\n              if self.api_base:\\n                # The user set the api_base. litellm needs this to be \"custom/{model}\"\\n                response = litellm.completion(\\n                  api_base=self.api_base,\\n                  model = \"custom/\" + self.model,\\n                  messages=messages,\\n                  functions=[function_schema],\\n                  stream=True,\\n                  temperature=self.temperature,\\n                )\\n              else:\\n                # Normal OpenAI call\\n                response = litellm.completion(\\n                  model=self.model,\\n                  messages=messages,\\n                  functions=[function_schema],\\n                  stream=True,\\n                  temperature=self.temperature,\\n                )' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='break\\n        except:\\n            if self.debug_mode:\\n              traceback.print_exc()\\n            error = traceback.format_exc()\\n            time.sleep(3)\\n      else:\\n        raise Exception(error)\\n            \\n    elif self.local:\\n      # Code-Llama\\n\\n\\n\\n      # Convert messages to prompt\\n      # (This only works if the first message is the only system message)\\n\\n      def messages_to_prompt(messages):\\n\\n\\n        for message in messages:\\n          # Happens if it immediatly writes code\\n          if \"role\" not in message:\\n            message[\"role\"] = \"assistant\"\\n\\n\\n        # Falcon prompt template\\n        if \"falcon\" in self.model.lower():\\n\\n          formatted_messages = \"\"\\n          for message in messages:\\n            formatted_messages += f\"{message[\\'role\\'].capitalize()}: {message[\\'content\\']}\\\\n\"\\n          formatted_messages = formatted_messages.strip()\\n\\n        else:\\n          # Llama prompt template\\n\\n          # Extracting the system prompt and initializing the formatted string with it.\\n          system_prompt = messages[0][\\'content\\']\\n          formatted_messages = f\"<s>[INST] <<SYS>>\\\\n{system_prompt}\\\\n<</SYS>>\\\\n\"\\n\\n          # Loop starting from the first user message\\n          for index, item in enumerate(messages[1:]):\\n              role = item[\\'role\\']\\n              content = item[\\'content\\']\\n\\n              if role == \\'user\\':\\n                  formatted_messages += f\"{content} [/INST] \"\\n              elif role == \\'function\\':\\n                  formatted_messages += f\"Output: {content} [/INST] \"\\n              elif role == \\'assistant\\':\\n                  formatted_messages += f\"{content} </s><s>[INST] \"\\n\\n          # Remove the trailing \\'<s>[INST] \\' from the final output\\n          if formatted_messages.endswith(\"<s>[INST] \"):\\n              formatted_messages = formatted_messages[:-10]\\n\\n        return formatted_messages\\n\\n      prompt = messages_to_prompt(messages)\\n      # Lmao i can\\'t believe this works (it does need this btw)\\n      if messages[-1][\"role\"] != \"function\":\\n        prompt += \"Let\\'s explore this. By the way, I can run code on your machine by writing the code in a markdown code block. This works for shell, javascript, python, R, and applescript. I\\'m going to try to do this for your task. Anyway, \"\\n      elif messages[-1][\"role\"] == \"function\" and messages[-1][\"content\"] != \"No output\":\\n        prompt += \"Given the output of the code I just ran, \"\\n      elif messages[-1][\"role\"] == \"function\" and messages[-1][\"content\"] == \"No output\":\\n        prompt += \"Given the fact that the code I just ran produced no output, \"\\n\\n\\n      if self.debug_mode:\\n        # we have to use builtins bizarrely! because rich.print interprets \"[INST]\" as something meaningful\\n        import builtins\\n        builtins.print(\"TEXT PROMPT SEND TO LLM:\\\\n\", prompt)\\n\\n      # Run Code-Llama' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Run Code-Llama\\n\\n      response = self.llama_instance(\\n        prompt,\\n        stream=True,\\n        temperature=self.temperature,\\n        stop=[\"</s>\"],\\n        max_tokens=750 # context window is set to 1800, messages are trimmed to 1000... 700 seems nice\\n      )\\n\\n    # Initialize message, function call trackers, and active block\\n    self.messages.append({})\\n    in_function_call = False\\n    llama_function_call_finished = False\\n    self.active_block = None\\n\\n    for chunk in response:\\n      if self.use_azure and (\\'choices\\' not in chunk or len(chunk[\\'choices\\']) == 0):\\n        # Azure OpenAI Service may return empty chunk\\n        continue\\n\\n      if self.local:\\n        if \"content\" not in messages[-1]:\\n          # This is the first chunk. We\\'ll need to capitalize it, because our prompt ends in a \", \"\\n          chunk[\"choices\"][0][\"text\"] = chunk[\"choices\"][0][\"text\"].capitalize()\\n          # We\\'ll also need to add \"role: assistant\", CodeLlama will not generate this\\n          messages[-1][\"role\"] = \"assistant\"\\n        delta = {\"content\": chunk[\"choices\"][0][\"text\"]}\\n      else:\\n        delta = chunk[\"choices\"][0][\"delta\"]\\n\\n      # Accumulate deltas into the last message in messages\\n      self.messages[-1] = merge_deltas(self.messages[-1], delta)\\n\\n      # Check if we\\'re in a function call\\n      if not self.local:\\n        condition = \"function_call\" in self.messages[-1]\\n      elif self.local:\\n        # Since Code-Llama can\\'t call functions, we just check if we\\'re in a code block.\\n        # This simply returns true if the number of \"```\" in the message is odd.\\n        if \"content\" in self.messages[-1]:\\n          condition = self.messages[-1][\"content\"].count(\"```\") % 2 == 1\\n        else:\\n          # If it hasn\\'t made \"content\" yet, we\\'re certainly not in a function call.\\n          condition = False\\n\\n      if condition:\\n        # We are in a function call.\\n\\n        # Check if we just entered a function call\\n        if in_function_call == False:\\n\\n          # If so, end the last block,\\n          self.end_active_block()\\n\\n          # Print newline if it was just a code block or user message\\n          # (this just looks nice)\\n          last_role = self.messages[-2][\"role\"]\\n          if last_role == \"user\" or last_role == \"function\":\\n            print()\\n\\n          # then create a new code block\\n          self.active_block = CodeBlock()\\n\\n        # Remember we\\'re in a function_call\\n        in_function_call = True\\n\\n        # Now let\\'s parse the function\\'s arguments:' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='if not self.local:\\n          # gpt-4\\n          # Parse arguments and save to parsed_arguments, under function_call\\n          if \"arguments\" in self.messages[-1][\"function_call\"]:\\n            arguments = self.messages[-1][\"function_call\"][\"arguments\"]\\n            new_parsed_arguments = parse_partial_json(arguments)\\n            if new_parsed_arguments:\\n              # Only overwrite what we have if it\\'s not None (which means it failed to parse)\\n              self.messages[-1][\"function_call\"][\\n                \"parsed_arguments\"] = new_parsed_arguments\\n\\n        elif self.local:\\n          # Code-Llama\\n          # Parse current code block and save to parsed_arguments, under function_call\\n          if \"content\" in self.messages[-1]:\\n\\n            content = self.messages[-1][\"content\"]\\n\\n            if \"```\" in content:\\n              # Split by \"```\" to get the last open code block\\n              blocks = content.split(\"```\")\\n\\n              current_code_block = blocks[-1]\\n\\n              lines = current_code_block.split(\"\\\\n\")\\n\\n              if content.strip() == \"```\": # Hasn\\'t outputted a language yet\\n                language = None\\n              else:\\n                if lines[0] != \"\":\\n                  language = lines[0].strip()\\n                else:\\n                  language = \"python\"\\n                  # In anticipation of its dumbassery let\\'s check if \"pip\" is in there\\n                  if len(lines) > 1:\\n                    if lines[1].startswith(\"pip\"):\\n                      language = \"shell\"\\n\\n              # Join all lines except for the language line\\n              code = \\'\\\\n\\'.join(lines[1:]).strip(\"` \\\\n\")\\n\\n              arguments = {\"code\": code}\\n              if language: # We only add this if we have it-- the second we have it, an interpreter gets fired up (I think? maybe I\\'m wrong)\\n                if language == \"bash\":\\n                  language = \"shell\"\\n                arguments[\"language\"] = language\\n\\n            # Code-Llama won\\'t make a \"function_call\" property for us to store this under, so:\\n            if \"function_call\" not in self.messages[-1]:\\n              self.messages[-1][\"function_call\"] = {}\\n\\n            self.messages[-1][\"function_call\"][\"parsed_arguments\"] = arguments\\n\\n      else:\\n        # We are not in a function call.\\n\\n        # Check if we just left a function call\\n        if in_function_call == True:\\n\\n          if self.local:\\n            # This is the same as when gpt-4 gives finish_reason as function_call.\\n            # We have just finished a code block, so now we should run it.\\n            llama_function_call_finished = True\\n\\n        # Remember we\\'re not in a function_call\\n        in_function_call = False\\n\\n        # If there\\'s no active block,\\n        if self.active_block == None:\\n\\n          # Create a message block\\n          self.active_block = MessageBlock()\\n\\n      # Update active_block\\n      self.active_block.update_from_message(self.messages[-1])' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='# Check if we\\'re finished\\n      if chunk[\"choices\"][0][\"finish_reason\"] or llama_function_call_finished:\\n        if chunk[\"choices\"][\\n            0][\"finish_reason\"] == \"function_call\" or llama_function_call_finished:\\n          # Time to call the function!\\n          # (Because this is Open Interpreter, we only have one function.)\\n\\n          if self.debug_mode:\\n            print(\"Running function:\")\\n            print(self.messages[-1])\\n            print(\"---\")\\n\\n          # Ask for user confirmation to run code\\n          if self.auto_run == False:\\n\\n            # End the active block so you can run input() below it\\n            # Save language and code so we can create a new block in a moment\\n            self.active_block.end()\\n            language = self.active_block.language\\n            code = self.active_block.code\\n\\n            # Prompt user\\n            response = input(\"  Would you like to run this code? (y/n)\\\\n\\\\n  \")\\n            print(\"\")  # <- Aesthetic choice\\n\\n            if response.strip().lower() == \"y\":\\n              # Create a new, identical block where the code will actually be run\\n              self.active_block = CodeBlock()\\n              self.active_block.language = language\\n              self.active_block.code = code\\n\\n            else:\\n              # User declined to run code.\\n              self.active_block.end()\\n              self.messages.append({\\n                \"role\":\\n                \"function\",\\n                \"name\":\\n                \"run_code\",\\n                \"content\":\\n                \"User decided not to run this code.\"\\n              })\\n              return\\n\\n          # If we couldn\\'t parse its arguments, we need to try again.\\n          if not self.local and \"parsed_arguments\" not in self.messages[-1][\"function_call\"]:\\n\\n            # After collecting some data via the below instruction to users,\\n            # This is the most common failure pattern: https://github.com/KillianLucas/open-interpreter/issues/41\\n\\n            # print(\"> Function call could not be parsed.\\\\n\\\\nPlease open an issue on Github (openinterpreter.com, click Github) and paste the following:\")\\n            # print(\"\\\\n\", self.messages[-1][\"function_call\"], \"\\\\n\")\\n            # time.sleep(2)\\n            # print(\"Informing the language model and continuing...\")\\n\\n            # Since it can\\'t really be fixed without something complex,\\n            # let\\'s just berate the LLM then go around again.\\n\\n            self.messages.append({\\n              \"role\": \"function\",\\n              \"name\": \"run_code\",\\n              \"content\": \"\"\"Your function call could not be parsed. Please use ONLY the `run_code` function, which takes two parameters: `code` and `language`. Your response should be formatted as a JSON.\"\"\"\\n            })\\n\\n            self.respond()\\n            return' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='self.respond()\\n            return\\n\\n          # Create or retrieve a Code Interpreter for this language\\n          language = self.messages[-1][\"function_call\"][\"parsed_arguments\"][\\n            \"language\"]\\n          if language not in self.code_interpreters:\\n            self.code_interpreters[language] = CodeInterpreter(language, self.debug_mode)\\n          code_interpreter = self.code_interpreters[language]\\n\\n          # Let this Code Interpreter control the active_block\\n          code_interpreter.active_block = self.active_block\\n          code_interpreter.run()\\n\\n          # End the active_block\\n          self.active_block.end()\\n\\n          # Append the output to messages\\n          # Explicitly tell it if there was no output (sometimes \"\" = hallucinates output)\\n          self.messages.append({\\n            \"role\": \"function\",\\n            \"name\": \"run_code\",\\n            \"content\": self.active_block.output if self.active_block.output else \"No output\"\\n          })\\n\\n          # Go around again\\n          self.respond()\\n\\n        if chunk[\"choices\"][0][\"finish_reason\"] != \"function_call\":\\n          # Done!\\n\\n          # Code Llama likes to output \"###\" at the end of every message for some reason\\n          if self.local and \"content\" in self.messages[-1]:\\n            self.messages[-1][\"content\"] = self.messages[-1][\"content\"].strip().rstrip(\"#\")\\n            self.active_block.update_from_message(self.messages[-1])\\n            time.sleep(0.1)\\n\\n          self.active_block.end()\\n          return' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='\"\"\"\\nRight off the bat, to any contributors (a message from Killian):\\n\\nFirst of all, THANK YOU. Open Interpreter is ALIVE, ALL OVER THE WORLD because of YOU.\\n\\nWhile this project is rapidly growing, I\\'ve decided it\\'s best for us to allow some technical debt.\\n\\nThe code here has duplication. It has imports in weird places. It has been spaghettified to add features more quickly.\\n\\nIn my opinion **this is critical** to keep up with the pace of demand for this project.\\n\\nAt the same time, I plan on pushing a significant re-factor of `interpreter.py` and `code_interpreter.py` ~ September 11th.\\n\\nAfter the re-factor, Open Interpreter\\'s source code will be much simpler, and much more fun to dive into.\\n\\nEspecially if you have ideas and **EXCITEMENT** about the future of this project, chat with me on discord: https://discord.gg/6p3fD6rBVm\\n\\n- killian\\n\"\"\"\\n\\nfrom .cli import cli\\nfrom .utils import merge_deltas, parse_partial_json\\nfrom .message_block import MessageBlock\\nfrom .code_block import CodeBlock\\nfrom .code_interpreter import CodeInterpreter\\nfrom .get_hf_llm import get_hf_llm\\n\\n\\nimport os\\nimport time\\nimport traceback\\nimport json\\nimport platform\\nimport openai\\nimport litellm\\n\\nimport getpass\\nimport requests\\nimport readline\\nimport tokentrim as tt\\nfrom rich import print\\nfrom rich.markdown import Markdown\\nfrom rich.rule import Rule\\n\\n# Function schema for gpt-4\\nfunction_schema = {\\n  \"name\": \"run_code\",\\n  \"description\":\\n  \"Executes code on the user\\'s machine and returns the output\",\\n  \"parameters\": {\\n    \"type\": \"object\",\\n    \"properties\": {\\n      \"language\": {\\n        \"type\": \"string\",\\n        \"description\":\\n        \"The programming language\",\\n        \"enum\": [\"python\", \"R\", \"shell\", \"applescript\", \"javascript\", \"html\"]\\n      },\\n      \"code\": {\\n        \"type\": \"string\",\\n        \"description\": \"The code to execute\"\\n      }\\n    },\\n    \"required\": [\"language\", \"code\"]\\n  },\\n}\\n\\n# Message for when users don\\'t have an OpenAI API key.\\nmissing_api_key_message = \"\"\"> OpenAI API key not found\\n\\nTo use `GPT-4` (recommended) please provide an OpenAI API key.\\n\\nTo use `Code-Llama` (free but less capable) press `enter`.\\n\"\"\"\\n\\n# Message for when users don\\'t have an OpenAI API key.\\nmissing_azure_info_message = \"\"\"> Azure OpenAI Service API info not found\\n\\nTo use `GPT-4` (recommended) please provide an Azure OpenAI API key, a API base, a deployment name and a API version.\\n\\nTo use `Code-Llama` (free but less capable) press `enter`.\\n\"\"\"\\n\\nconfirm_mode_message = \"\"\"\\n**Open Interpreter** will require approval before running code. Use `interpreter -y` to bypass this.\\n\\nPress `CTRL-C` to exit.\\n\"\"\"\\n\\n\\n# Code for: class Interpreter:' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\interpreter.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='from rich.console import Console\\nfrom rich.live import Live\\nfrom rich.panel import Panel\\nfrom rich.markdown import Markdown\\nfrom rich.box import MINIMAL\\nimport re\\n\\n\\nclass MessageBlock:\\n\\n  def __init__(self):\\n    self.live = Live(auto_refresh=False, console=Console())\\n    self.live.start()\\n    self.content = \"\"\\n\\n  def update_from_message(self, message):\\n    self.content = message.get(\"content\", \"\")\\n    if self.content:\\n      self.refresh()\\n\\n  def end(self):\\n    self.refresh(cursor=False)\\n    self.live.stop()\\n\\n  def refresh(self, cursor=True):\\n    # De-stylize any code blocks in markdown,\\n    # to differentiate from our Code Blocks\\n    content = textify_markdown_code_blocks(self.content)\\n    \\n    if cursor:\\n      content += \"█\"\\n      \\n    markdown = Markdown(content.strip())\\n    panel = Panel(markdown, box=MINIMAL)\\n    self.live.update(panel)\\n    self.live.refresh()\\n\\n\\ndef textify_markdown_code_blocks(text):\\n  \"\"\"\\n  To distinguish CodeBlocks from markdown code, we simply turn all markdown code\\n  (like \\'```python...\\') into text code blocks (\\'```text\\') which makes the code black and white.\\n  \"\"\"\\n  replacement = \"```text\"\\n  lines = text.split(\\'\\\\n\\')\\n  inside_code_block = False\\n\\n  for i in range(len(lines)):\\n    # If the line matches ``` followed by optional language specifier\\n    if re.match(r\\'^```(\\\\w*)$\\', lines[i].strip()):\\n      inside_code_block = not inside_code_block\\n\\n      # If we just entered a code block, replace the marker\\n      if inside_code_block:\\n        lines[i] = replacement\\n\\n  return \\'\\\\n\\'.join(lines)' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\message_block.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='import json\\nimport re\\n\\ndef merge_deltas(original, delta):\\n    \"\"\"\\n    Pushes the delta into the original and returns that.\\n\\n    Great for reconstructing OpenAI streaming responses -> complete message objects.\\n    \"\"\"\\n    for key, value in delta.items():\\n        if isinstance(value, dict):\\n            if key not in original:\\n                original[key] = value\\n            else:\\n                merge_deltas(original[key], value)\\n        else:\\n            if key in original:\\n                original[key] += value\\n            else:\\n                original[key] = value\\n    return original\\n\\ndef parse_partial_json(s):\\n\\n    # Attempt to parse the string as-is.\\n    try:\\n        return json.loads(s)\\n    except json.JSONDecodeError:\\n        pass\\n  \\n    # Initialize variables.\\n    new_s = \"\"\\n    stack = []\\n    is_inside_string = False\\n    escaped = False\\n\\n    # Process each character in the string one at a time.\\n    for char in s:\\n        if is_inside_string:\\n            if char == \\'\"\\' and not escaped:\\n                is_inside_string = False\\n            elif char == \\'\\\\n\\' and not escaped:\\n                char = \\'\\\\\\\\n\\' # Replace the newline character with the escape sequence.\\n            elif char == \\'\\\\\\\\\\':\\n                escaped = not escaped\\n            else:\\n                escaped = False\\n        else:\\n            if char == \\'\"\\':\\n                is_inside_string = True\\n                escaped = False\\n            elif char == \\'{\\':\\n                stack.append(\\'}\\')\\n            elif char == \\'[\\':\\n                stack.append(\\']\\')\\n            elif char == \\'}\\' or char == \\']\\':\\n                if stack and stack[-1] == char:\\n                    stack.pop()\\n                else:\\n                    # Mismatched closing character; the input is malformed.\\n                    return None\\n        \\n        # Append the processed character to the new string.\\n        new_s += char\\n\\n    # If we\\'re still inside a string at the end of processing, we need to close the string.\\n    if is_inside_string:\\n        new_s += \\'\"\\'\\n\\n    # Close any remaining open structures in the reverse order that they were opened.\\n    for closing_char in reversed(stack):\\n        new_s += closing_char\\n\\n    # Attempt to parse the modified string as JSON.\\n    try:\\n        return json.loads(new_s)\\n    except json.JSONDecodeError:\\n        # If we still can\\'t parse the string as JSON, return None to indicate failure.\\n        return None' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      " page_content='from .interpreter import Interpreter\\nimport sys\\n\\n# This is done so when users `import interpreter`,\\n# they get an instance of interpreter:\\n\\nsys.modules[\"interpreter\"] = Interpreter()\\n\\n# **This is a controversial thing to do,**\\n# because perhaps modules ought to behave like modules.\\n\\n# But I think it saves a step, removes friction, and looks good.\\n\\n#     ____                      ____      __                            __           \\n#    / __ \\\\____  ___  ____     /  _/___  / /____  _________  ________  / /____  _____\\n#   / / / / __ \\\\/ _ \\\\/ __ \\\\    / // __ \\\\/ __/ _ \\\\/ ___/ __ \\\\/ ___/ _ \\\\/ __/ _ \\\\/ ___/\\n#  / /_/ / /_/ /  __/ / / /  _/ // / / / /_/  __/ /  / /_/ / /  /  __/ /_/  __/ /    \\n#  \\\\____/ .___/\\\\___/_/ /_/  /___/_/ /_/\\\\__/\\\\___/_/  / .___/_/   \\\\___/\\\\__/\\\\___/_/     \\n#      /_/                                         /_/' metadata={'source': '..\\\\..\\\\..\\\\open-interpreter\\\\interpreter\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n"
     ]
    }
   ],
   "source": [
    "for  text in texts:\n",
    "    print(f' {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating embeddings:   0%|          | 0/9 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\brian_jupyter\\learn_github\\learn_open_chat.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings(disallowed_special\u001b[39m=\u001b[39m())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m db \u001b[39m=\u001b[39m DeepLake(dataset_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhub://\u001b[39m\u001b[39m{\u001b[39;00musername\u001b[39m}\u001b[39;00m\u001b[39m/interpreter\u001b[39m\u001b[39m\"\u001b[39m, embedding\u001b[39m=\u001b[39membeddings,token\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m db\u001b[39m.\u001b[39;49madd_documents(texts)\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\langchain\\schema\\vectorstore.py:104\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    103\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_texts(texts, metadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\langchain\\vectorstores\\deeplake.py:226\u001b[0m, in \u001b[0;36mDeepLake.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(texts) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    224\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`texts` parameter shouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be empty.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 226\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49madd(\n\u001b[0;32m    227\u001b[0m     text\u001b[39m=\u001b[39;49mtexts,\n\u001b[0;32m    228\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[0;32m    229\u001b[0m     embedding_data\u001b[39m=\u001b[39;49mtexts,\n\u001b[0;32m    230\u001b[0m     embedding_tensor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39membedding\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    231\u001b[0m     embedding_function\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    232\u001b[0m     return_ids\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    233\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    234\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\deeplake\\core\\vectorstore\\deeplake_vectorstore.py:340\u001b[0m, in \u001b[0;36mVectorStore.add\u001b[1;34m(self, embedding_function, embedding_data, embedding_tensor, return_ids, rate_limiter, batch_byte_size, **tensors)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m index\u001b[39m.\u001b[39mcheck_vdb_indexes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset):\n\u001b[0;32m    338\u001b[0m     index_regeneration \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 340\u001b[0m dataset_utils\u001b[39m.\u001b[39;49mextend_or_ingest_dataset(\n\u001b[0;32m    341\u001b[0m     processed_tensors\u001b[39m=\u001b[39;49mprocessed_tensors,\n\u001b[0;32m    342\u001b[0m     dataset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset,\n\u001b[0;32m    343\u001b[0m     embedding_function\u001b[39m=\u001b[39;49membedding_function,\n\u001b[0;32m    344\u001b[0m     embedding_data\u001b[39m=\u001b[39;49membedding_data,\n\u001b[0;32m    345\u001b[0m     embedding_tensor\u001b[39m=\u001b[39;49membedding_tensor,\n\u001b[0;32m    346\u001b[0m     batch_byte_size\u001b[39m=\u001b[39;49mbatch_byte_size,\n\u001b[0;32m    347\u001b[0m     rate_limiter\u001b[39m=\u001b[39;49mrate_limiter,\n\u001b[0;32m    348\u001b[0m )\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m utils\u001b[39m.\u001b[39mindex_used(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexec_option):\n\u001b[0;32m    351\u001b[0m     index\u001b[39m.\u001b[39mindex_cache_cleanup(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset)\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:552\u001b[0m, in \u001b[0;36mextend_or_ingest_dataset\u001b[1;34m(processed_tensors, dataset, embedding_function, embedding_tensor, embedding_data, batch_byte_size, rate_limiter, index_regeneration)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextend_or_ingest_dataset\u001b[39m(\n\u001b[0;32m    542\u001b[0m     processed_tensors,\n\u001b[0;32m    543\u001b[0m     dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m ):\n\u001b[0;32m    551\u001b[0m     \u001b[39m# TODO: Add back the old logic with checkpointing after indexing is fixed\u001b[39;00m\n\u001b[1;32m--> 552\u001b[0m     extend(\n\u001b[0;32m    553\u001b[0m         embedding_function,\n\u001b[0;32m    554\u001b[0m         embedding_data,\n\u001b[0;32m    555\u001b[0m         embedding_tensor,\n\u001b[0;32m    556\u001b[0m         processed_tensors,\n\u001b[0;32m    557\u001b[0m         dataset,\n\u001b[0;32m    558\u001b[0m         batch_byte_size,\n\u001b[0;32m    559\u001b[0m         rate_limiter,\n\u001b[0;32m    560\u001b[0m         index_regeneration\u001b[39m=\u001b[39;49mindex_regeneration,\n\u001b[0;32m    561\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:454\u001b[0m, in \u001b[0;36mextend\u001b[1;34m(embedding_function, embedding_data, embedding_tensor, processed_tensors, dataset, batch_byte_size, rate_limiter, index_regeneration)\u001b[0m\n\u001b[0;32m    449\u001b[0m data_iterator \u001b[39m=\u001b[39m data_iteratot_factory(\n\u001b[0;32m    450\u001b[0m     data, func, batch_byte_size, rate_limiter\n\u001b[0;32m    451\u001b[0m )\n\u001b[0;32m    452\u001b[0m embedded_data \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 454\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(\n\u001b[0;32m    455\u001b[0m     data_iterator, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_iterator), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcreating embeddings\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m ):\n\u001b[0;32m    457\u001b[0m     embedded_data\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m    459\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:488\u001b[0m, in \u001b[0;36mDataIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m    487\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_itr)\n\u001b[1;32m--> 488\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(batch)\n\u001b[0;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    490\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\langchain\\embeddings\\openai.py:490\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\.venv\\Lib\\site-packages\\langchain\\embeddings\\openai.py:386\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_empty \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(batched_embeddings[i]) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    385\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     results[indices[i]]\u001b[39m.\u001b[39mappend(batched_embeddings[i])\n\u001b[0;32m    387\u001b[0m     num_tokens_in_batch[indices[i]]\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(tokens[i]))\n\u001b[0;32m    389\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(texts)):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "username=\"huangliang\"\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())\n",
    "db = DeepLake(dataset_path=f\"hub://{username}/interpreter\", embedding=embeddings,token=token)\n",
    "db.add_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://huangliang/open_chat already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path='hub://huangliang/interpreter', read_only=True,embedding=embeddings,token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\brian_jupyter\\learn_github\\learn_open_chat.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m retriever\u001b[39m=\u001b[39mdb\u001b[39m.\u001b[39mas_retriever()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#distance_metric计算两个的点的距离\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m retriever\u001b[39m.\u001b[39msearch_kwargs[\u001b[39m'\u001b[39m\u001b[39mdistance_metric\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcos\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "retriever=db.as_retriever()\n",
    "#distance_metric计算两个的点的距离\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "#fetch_k 从文档中要检索的数据点的数据\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "#k 返回相似度最高的向量的数量  \n",
    "retriever.search_kwargs['k'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\brian_jupyter\\learn_github\\learn_open_chat.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalRetrievalChain\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m ChatOpenAI(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-16k\u001b[39m\u001b[39m\"\u001b[39m,openai_api_key\u001b[39m=\u001b[39mapi_key,temperature\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(model, retriever\u001b[39m=\u001b[39mretriever)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\",openai_api_key=api_key,temperature=0.1)\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "specified dataset is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hh\\Desktop\\brian\\code\\brain-search\\brian_jupyter\\learn_github\\learn_open_chat.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m chat_history \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m questions:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     result \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m:question,\u001b[39m\"\u001b[39;49m\u001b[39mchat_history\u001b[39;49m\u001b[39m\"\u001b[39;49m:chat_history})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     chat_history\u001b[39m.\u001b[39mappend((question,result[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hh/Desktop/brian/code/brain-search/brian_jupyter/learn_github/learn_open_chat.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQ:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mquestion\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    283\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    284\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    286\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    270\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    271\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m )\n\u001b[0;32m    274\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    277\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    280\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    281\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:134\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    130\u001b[0m accepts_run_manager \u001b[39m=\u001b[39m (\n\u001b[0;32m    131\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs)\u001b[39m.\u001b[39mparameters\n\u001b[0;32m    132\u001b[0m )\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager:\n\u001b[1;32m--> 134\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_docs(new_question, inputs, run_manager\u001b[39m=\u001b[39;49m_run_manager)\n\u001b[0;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(new_question, inputs)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:286\u001b[0m, in \u001b[0;36mConversationalRetrievalChain._get_docs\u001b[1;34m(self, question, inputs, run_manager)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_docs\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     question: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[0;32m    284\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    285\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretriever\u001b[39m.\u001b[39;49mget_relevant_documents(\n\u001b[0;32m    287\u001b[0m         question, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[0;32m    288\u001b[0m     )\n\u001b[0;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reduce_tokens_below_limit(docs)\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\retriever.py:208\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    207\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    209\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_end(\n\u001b[0;32m    211\u001b[0m         result,\n\u001b[0;32m    212\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\retriever.py:201\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 201\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[0;32m    202\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\vectorstores\\base.py:564\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_relevant_documents\u001b[39m(\n\u001b[0;32m    561\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m    562\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 564\u001b[0m         docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49msimilarity_search(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_kwargs)\n\u001b[0;32m    565\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity_score_threshold\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    566\u001b[0m         docs_and_similarities \u001b[39m=\u001b[39m (\n\u001b[0;32m    567\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m    568\u001b[0m                 query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_kwargs\n\u001b[0;32m    569\u001b[0m             )\n\u001b[0;32m    570\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\vectorstores\\deeplake.py:469\u001b[0m, in \u001b[0;36mDeepLake.similarity_search\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    420\u001b[0m     k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m,\n\u001b[0;32m    421\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    422\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    423\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[39m    Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[39m        List[Document]: List of Documents most similar to the query vector.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_search(\n\u001b[0;32m    470\u001b[0m         query\u001b[39m=\u001b[39;49mquery,\n\u001b[0;32m    471\u001b[0m         k\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m    472\u001b[0m         use_maximal_marginal_relevance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    473\u001b[0m         return_score\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    474\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    475\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\vectorstores\\deeplake.py:377\u001b[0m, in \u001b[0;36mDeepLake._search\u001b[1;34m(self, query, embedding, embedding_function, k, distance_metric, use_maximal_marginal_relevance, fetch_k, filter, return_score, exec_option, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(embedding\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    375\u001b[0m         embedding \u001b[39m=\u001b[39m embedding[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 377\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49msearch(\n\u001b[0;32m    378\u001b[0m     embedding\u001b[39m=\u001b[39;49membedding,\n\u001b[0;32m    379\u001b[0m     k\u001b[39m=\u001b[39;49mfetch_k \u001b[39mif\u001b[39;49;00m use_maximal_marginal_relevance \u001b[39melse\u001b[39;49;00m k,\n\u001b[0;32m    380\u001b[0m     distance_metric\u001b[39m=\u001b[39;49mdistance_metric,\n\u001b[0;32m    381\u001b[0m     \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m,\n\u001b[0;32m    382\u001b[0m     exec_option\u001b[39m=\u001b[39;49mexec_option,\n\u001b[0;32m    383\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39membedding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    384\u001b[0m )\n\u001b[0;32m    386\u001b[0m scores \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    387\u001b[0m embeddings \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\core\\vectorstore\\deeplake_vectorstore.py:427\u001b[0m, in \u001b[0;36mVectorStore.search\u001b[1;34m(self, embedding_data, embedding_function, embedding, k, distance_metric, query, filter, exec_option, embedding_tensor, return_tensors, return_view)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m query \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     query_emb \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39mget_embedding(\n\u001b[0;32m    423\u001b[0m         embedding,\n\u001b[0;32m    424\u001b[0m         embedding_data,\n\u001b[0;32m    425\u001b[0m         embedding_function\u001b[39m=\u001b[39membedding_function \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_function,\n\u001b[0;32m    426\u001b[0m     )\n\u001b[1;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m vector_search\u001b[39m.\u001b[39;49msearch(\n\u001b[0;32m    428\u001b[0m     query\u001b[39m=\u001b[39;49mquery,\n\u001b[0;32m    429\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m    430\u001b[0m     \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m,\n\u001b[0;32m    431\u001b[0m     query_embedding\u001b[39m=\u001b[39;49mquery_emb,\n\u001b[0;32m    432\u001b[0m     k\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m    433\u001b[0m     distance_metric\u001b[39m=\u001b[39;49mdistance_metric,\n\u001b[0;32m    434\u001b[0m     exec_option\u001b[39m=\u001b[39;49mexec_option,\n\u001b[0;32m    435\u001b[0m     deeplake_dataset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset,\n\u001b[0;32m    436\u001b[0m     embedding_tensor\u001b[39m=\u001b[39;49membedding_tensor,\n\u001b[0;32m    437\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    438\u001b[0m     return_view\u001b[39m=\u001b[39;49mreturn_view,\n\u001b[0;32m    439\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\vector_search.py:51\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(k, distance_metric, exec_option, deeplake_dataset, return_tensors, query, logger, filter, query_embedding, embedding_tensor, return_view)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(\n\u001b[0;32m     20\u001b[0m     k: Optional[\u001b[39mint\u001b[39m],\n\u001b[0;32m     21\u001b[0m     distance_metric: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     return_view: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Dict, DeepLakeDataset]:\n\u001b[0;32m     32\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Searching function\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m        query (Optional[str]) - TQL Query string for direct evaluation, without application of additional filters or vector search.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m        return_view (Bool): Return a Deep Lake dataset view that satisfied the search parameters, instead of a dictinary with data. Defaults to False.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m EXEC_OPTION_TO_SEARCH_TYPE[exec_option](\n\u001b[0;32m     52\u001b[0m         query\u001b[39m=\u001b[39;49mquery,\n\u001b[0;32m     53\u001b[0m         query_emb\u001b[39m=\u001b[39;49mquery_embedding,\n\u001b[0;32m     54\u001b[0m         exec_option\u001b[39m=\u001b[39;49mexec_option,\n\u001b[0;32m     55\u001b[0m         dataset\u001b[39m=\u001b[39;49mdeeplake_dataset,\n\u001b[0;32m     56\u001b[0m         logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m     57\u001b[0m         \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m,\n\u001b[0;32m     58\u001b[0m         embedding_tensor\u001b[39m=\u001b[39;49membedding_tensor,\n\u001b[0;32m     59\u001b[0m         distance_metric\u001b[39m=\u001b[39;49mdistance_metric,\n\u001b[0;32m     60\u001b[0m         k\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m     61\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m     62\u001b[0m         return_view\u001b[39m=\u001b[39;49mreturn_view,\n\u001b[0;32m     63\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\python\\vector_search.py:27\u001b[0m, in \u001b[0;36mvector_search\u001b[1;34m(query, query_emb, exec_option, dataset, logger, filter, embedding_tensor, distance_metric, k, return_tensors, return_view)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m query \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     24\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUser-specified TQL queries are not supported for exec_option=\u001b[39m\u001b[39m{\u001b[39;00mexec_option\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[1;32m---> 27\u001b[0m view \u001b[39m=\u001b[39m filter_utils\u001b[39m.\u001b[39;49mattribute_based_filtering_python(dataset, \u001b[39mfilter\u001b[39;49m)\n\u001b[0;32m     29\u001b[0m return_data \u001b[39m=\u001b[39m {}\n\u001b[0;32m     31\u001b[0m \u001b[39m# Only fetch embeddings and run the search algorithm if an embedding query is specified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\filter\\filter.py:29\u001b[0m, in \u001b[0;36mattribute_based_filtering_python\u001b[1;34m(view, filter)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattribute_based_filtering_python\u001b[39m(\n\u001b[0;32m     26\u001b[0m     view, \u001b[39mfilter\u001b[39m: Optional[Union[Dict, Callable]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     27\u001b[0m ):\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(view) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mspecified dataset is empty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mfilter\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mfilter\u001b[39m, \u001b[39mdict\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: specified dataset is empty"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"\"\"\n",
    "    请为我提供有关OpenChat项目的信息。\n",
    "    我已经在本地安装了该项目的代码，并希望了解如何使用OpenChat爬取网站内容并定期自动更新其知识库。\n",
    "    \"\"\"\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\":question,\"chat_history\":chat_history})\n",
    "    chat_history.append((question,result['answer']))\n",
    "    print(f\"Q:\\n {question} \\n\")\n",
    "    print(f\"A:\\n {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question,chat_history):\n",
    "    res = qa({\"question\":question,\"chat_history\":chat_history})\n",
    "    print(f\"Q:\\n {question} \\n\")\n",
    "    print(f\"A:\\n {res['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " What's the main programming language used in gpt-engineer? \n",
      "\n",
      "A:\n",
      " The primary programming language used in GPT-Engineer is Python. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"What's the main programming language used in gpt-engineer?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " What's python class to generate the prompt in gpt-engineer \n",
      "\n",
      "A:\n",
      " The Python class used to generate prompts in GPT-Engineer is the `AI` class. It serves as the main interface to the GPT-4 model and provides methods to start a conversation with the model, continue an existing conversation, and format system and user messages. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"What's python class to generate the prompt in gpt-engineer\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Can tell me the file in 'preprompts' floder of gpt-engineer? \n",
      "\n",
      "A:\n",
      " In the 'preprompts' folder of GPT-Engineer, you can find the following files:\n",
      "\n",
      "1. fix_code\n",
      "2. generate\n",
      "3. philosophy\n",
      "4. qa\n",
      "5. respec\n",
      "6. spec\n",
      "7. unit_tests\n",
      "8. use_feedback\n",
      "\n",
      "These files contain predefined prompts that guide the AI in performing different tasks. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Can tell me the file in 'preprompts' floder of gpt-engineer?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me the order of gpt-engineer use the prompt file in 'preprompts' floder? \n",
      "\n",
      "A:\n",
      " The order of using the prompt file in the 'preprompts' folder for GPT-Engineer is as follows:\n",
      "\n",
      "1. Fix Code (`gpt_engineer/preprompts/fix_code`)\n",
      "2. Generate (`gpt_engineer/preprompts/generate`)\n",
      "3. Philosophy (`gpt_engineer/preprompts/philosophy`)\n",
      "4. QA (`gpt_engineer/preprompts/qa`)\n",
      "5. Respec (`gpt_engineer/preprompts/respec`)\n",
      "6. Spec (`gpt_engineer/preprompts/spec`)\n",
      "7. Unit Tests (`gpt_engineer/preprompts/unit_tests`)\n",
      "8. Use Feedback (`gpt_engineer/preprompts/use_feedback`)\n",
      "\n",
      "This order represents a sequence of different tasks that the AI can perform based on the instructions provided in each prompt. It starts with fixing code, followed by generating code, considering best practices, seeking clarification, reviewing specifications, writing unit tests, and utilizing user feedback for code generation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me the order of gpt-engineer use the prompt file in 'preprompts' floder?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me the Python class of gpt-engineer use the prompt file in 'preprompts' floder? \n",
      "\n",
      "A:\n",
      " The Python class used by gpt-engineer to access the prompt file in the 'preprompts' folder is the `DB` class. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me the Python class of gpt-engineer use the prompt file in 'preprompts' floder?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me what dose DB.py do in gpt-engineer ? \n",
      "\n",
      "A:\n",
      " The purpose of `DB.py` in GPT-Engineer is to represent a simple database that stores its data as files in a directory. It serves as a key-value store, where keys are filenames and values are file contents. The `DB` class provides methods to check if a key (filename) exists in the database, get the value (file content) associated with a key, and set the value associated with a key. The `DB` class is responsible for managing the data used by the GPT-Engineer system. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me what dose DB.py do in gpt-engineer ?\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " Tell me Where dose DB.py store the data in gpt-engineer ? \n",
      "\n",
      "A:\n",
      " The `DB.py` file in `gpt-engineer` represents a simple database that stores its data as files in a directory. The directory where the data is stored can be specified when creating an instance of the `DB` class by providing a path argument. If the directory does not already exist, the `DB` class will create it. Each file in the directory represents a key-value pair, where the filename serves as the key and the file content serves as the value. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask(\"Tell me Where dose DB.py store the data in gpt-engineer ?\",chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
