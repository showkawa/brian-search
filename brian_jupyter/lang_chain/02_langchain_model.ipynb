{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 安装langchain和openai\n",
    "%pip install langchain==0.0.270 openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model I/O</h3>\n",
    "<h5>1.Prompts</h5>\n",
    "<h5>2.Language models</h5>\n",
    "<h5>3.Output parsers</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load_dotenv 加载环境变量 即当前文件夹下的.env\n",
    "## 可以添加配置OPENAI_API_KEY=\"sk-xxx\"\n",
    "import os\n",
    "os.environ.clear()\n",
    "from dotenv import load_dotenv\n",
    "## 加载环境变量\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>1.1 PromptTemplate</h6>\n",
    "<h6>1.2 ChatPromptTemplate</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please tell me what is ModelScope?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'how to use langchain to work with Bilibili?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate \n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"please tell me what is {topic}?\")\n",
    "fm= prompt_template.format(topic=\"ModelScope\")\n",
    "print(fm)\n",
    "\n",
    "## 显示的指定input_variables\n",
    "prompt_tp2=PromptTemplate(template=\"how to use {tool} to work with {app}?\", input_variables=[\"tool\",\"app\"])\n",
    "prompt_tp2.format(tool=\"langchain\", app=\"Bilibili\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What is the difference between front-end and back-end development?', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chat prompt template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "tp =ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"you are an IT programming expert, when i ask your question you should give the accurate answer.\n",
    "    Don't make it up.if you don't know, just say i have no idea\n",
    "    \"\"\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "mes= tp.format_messages(\n",
    "question=\"how to use langchain to work with Bilibili\"\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,openai_api_key=os.environ.get('OPENAI_API_KEY'), model_name=\"gpt-3.5-turbo\",verbose=True)\n",
    "# llm(mes)\n",
    "\n",
    "#SystemMessage HumanMessage\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessage\n",
    "tp2 =ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"you are an IT programming expert, when i ask your question you should give the accurate answer.\n",
    "    Don't make it up.if you don't know, just say i have no idea\n",
    "    \"\"\"),\n",
    "    HumanMessage(content=\"{question}\")\n",
    "])\n",
    "mes2=tp2.format_messages(question=\"\"\"i want to use python3+langchain to write the automation tool that \n",
    "                    can connect with Bitbucket,Jira,Confluence and postman, can give me a solution\n",
    "                    \"\"\")\n",
    "llm(mes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>1.3 CustomPromptyTemplate</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom prompt template\n",
    "#https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template\n",
    "\n",
    "import inspect\n",
    "def get_source_code(function_name):\n",
    "    #Get the source code of the function\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "PROMPT=\"\"\"\\\n",
    "Given the function name and source code, generate an Chinese language explanation of the function.\n",
    "Function Name: {function_name}\n",
    "Source Code:\n",
    "{source_code}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate,BaseModel):\n",
    "    \"\"\"\n",
    "    A custom prompt template that takes in the function name as input, \n",
    "    and formats the prompt template to provide the source code of the function.\n",
    "    \"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"\n",
    "        Validate that the input variables are correct.\n",
    "        \"\"\"\n",
    "        if len(v) !=1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        #Get the source code of the function\n",
    "        source_code=get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        #Generate the prompt to be sent to the language model\n",
    "        prompt=PROMPT.format(function_name=kwargs[\"function_name\"].__name__\n",
    "                             ,source_code=source_code)\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self) -> str:\n",
    "        return \"function_explainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the function name and source code, generate an Chinese language explanation of the function.\n",
      "Function Name: get_source_code\n",
      "Source Code:\n",
      "def get_source_code(function_name):\n",
      "    #Get the source code of the function\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the custom prompt template\n",
    "f_e=FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "prompt=f_e.format(function_name=get_source_code)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>1.4 FewShotPromptTemplate</h6>\n",
    "<content>Few-shot prompt 是一种在大型语言模型（LLM）中使用的技术，它可以通过提供少量的示例来帮助模型理解任务并生成相应的内容。 Few-shot prompt 通常用于那些需要模型快速学习新任务或适应新领域的场景，例如文本分类、摘要、翻译、问答等。\n",
    "在这些场景中，Few-shot prompt 可以通过提供少量的输入-输出示例来帮助模型理解任务，并在不需要大量训练数据或重新训练模型的情况下快速生成相应的内容。这种方法在处理小数据集或需要快速适应新任务的场景中非常有用。</content>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples=[\n",
    "    {\n",
    "     \"question\":\"what's our team current Sprint JIRA task?\",\n",
    "     \"answer\":{\"tag\":\"JIRA\",\"title\":\"team current Sprint JIRA list\",\n",
    "               \"path\":\"/v1/jira/team1\",\"method\":\"GET\",\"requstBody\":{\"param\":1}} \n",
    "    },\n",
    "    {\n",
    "     \"question\":\"what's our team last Sprint JIRA task?\",\n",
    "     \"answer\":{\"tag\":\"JIRA\",\"title\":\"team last Sprint JIRA list\",\n",
    "               \"path\":\"/v1/jira/team1\",\"method\":\"GET\",\"requstBody\":{\"param\":2}} \n",
    "    },\n",
    "    {\n",
    "     \"question\":\"what's the William's JIRA task in current Sprint?\",\n",
    "     \"answer\":{\"tag\":\"JIRA\",\"title\":\"team member Sprint JIRA list\",\n",
    "               \"path\":\"/v1/jira/team1\",\"method\":\"GET\",\"requstBody\":{\"param\":3}} \n",
    "    },\n",
    "    {\n",
    "     \"question\":\"what's the TODO List JIRA task in current Sprint?\",\n",
    "     \"answer\":{\"tag\":\"JIRA\",\"title\":\"team current Sprint TODO JIRA list\",\n",
    "               \"path\":\"/v1/jira/team1\",\"method\":\"GET\",\"requstBody\":{\"param\":4}} \n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what's the TODO List JIRA task in current Sprint?\n",
      "{'tag': 'JIRA', 'title': 'team current Sprint TODO JIRA list', 'path': '/v1/jira/team1', 'method': 'GET', 'requstBody': {'param': 4}}\n"
     ]
    }
   ],
   "source": [
    "e_p=PromptTemplate(input_variables=[\"question\",\"answer\"],template=\"Question: {question}\\n{answer}\")\n",
    "print(e_p.format(**examples[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Feed examples and formatter to FewShotPromptTemplate\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prompt\u001b[39m=\u001b[39mFewShotPromptTemplate(\n\u001b[0;32m      3\u001b[0m     examples\u001b[39m=\u001b[39;49mexamples,\n\u001b[0;32m      4\u001b[0m     \u001b[39m# example_prompt=e_p,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39m# suffix=\"Question: {input}\",\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39m# input_variables=[\"input\"]\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(prompt)\n\u001b[0;32m      9\u001b[0m \u001b[39m#print(prompt.format(input=\"what's the Cancelled List JIRA task in current Sprint?\"))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\load\\serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\prompt_template.py:53\u001b[0m, in \u001b[0;36mBasePromptTemplate.validate_variable_names\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m@root_validator\u001b[39m()\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_variable_names\u001b[39m(\u001b[39mcls\u001b[39m, values: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[0;32m     52\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate variable names do not include restricted names.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m values[\u001b[39m\"\u001b[39;49m\u001b[39minput_variables\u001b[39;49m\u001b[39m\"\u001b[39;49m]:\n\u001b[0;32m     54\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     55\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have an input variable named \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, as it is used internally,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m please rename.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m         )\n\u001b[0;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m values[\u001b[39m\"\u001b[39m\u001b[39mpartial_variables\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input_variables'"
     ]
    }
   ],
   "source": [
    "# Feed examples and formatter to FewShotPromptTemplate\n",
    "prompt=FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    # example_prompt=e_p,\n",
    "    # suffix=\"Question: {input}\",\n",
    "    # input_variables=[\"input\"]\n",
    ")\n",
    "print(prompt)\n",
    "#print(prompt.format(input=\"what's the Cancelled List JIRA task in current Sprint?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
