{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 安装langchain和openai\n",
    "%pip install langchain==0.0.270 openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model I/O</h3>\n",
    "<h5>1.Prompts</h5>\n",
    "<h5>2.Language models</h5>\n",
    "<h5>3.Output parsers</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load_dotenv 加载环境变量 即当前文件夹下的.env\n",
    "## 可以添加配置OPENAI_API_KEY=\"sk-xxx\"\n",
    "import os\n",
    "# os.environ.clear()\n",
    "from dotenv import load_dotenv\n",
    "## 加载环境变量\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.1 PromptTemplate</h5>\n",
    "<h5>1.2 ChatPromptTemplate</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.25) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please tell me what is ModelScope?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'how to use langchain to work with Bilibili?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate \n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"please tell me what is {topic}?\")\n",
    "fm= prompt_template.format(topic=\"ModelScope\")\n",
    "print(fm)\n",
    "\n",
    "## 显示的指定input_variables\n",
    "prompt_tp2=PromptTemplate(template=\"how to use {tool} to work with {app}?\", input_variables=[\"tool\",\"app\"])\n",
    "prompt_tp2.format(tool=\"langchain\", app=\"Bilibili\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What is the difference between front-end and back-end development?', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chat prompt template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "tp =ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"you are an IT programming expert, when i ask your question you should give the accurate answer.\n",
    "    Don't make it up.if you don't know, just say i have no idea\n",
    "    \"\"\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "mes= tp.format_messages(\n",
    "question=\"how to use langchain to work with Bilibili\"\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,openai_api_key=os.environ.get('OPENAI_API_KEY'), model_name=\"gpt-3.5-turbo\",verbose=True)\n",
    "# llm(mes)\n",
    "\n",
    "#SystemMessage HumanMessage\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessage\n",
    "tp2 =ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"you are an IT programming expert, when i ask your question you should give the accurate answer.\n",
    "    Don't make it up.if you don't know, just say i have no idea\n",
    "    \"\"\"),\n",
    "    HumanMessage(content=\"{question}\")\n",
    "])\n",
    "mes2=tp2.format_messages(question=\"\"\"i want to use python3+langchain to write the automation tool that \n",
    "                    can connect with Bitbucket,Jira,Confluence and postman, can give me a solution\n",
    "                    \"\"\")\n",
    "llm(mes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.3 CustomPromptyTemplate</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom prompt template\n",
    "#https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template\n",
    "\n",
    "import inspect\n",
    "def get_source_code(function_name):\n",
    "    #Get the source code of the function\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "PROMPT=\"\"\"\\\n",
    "Given the function name and source code, generate an Chinese language explanation of the function.\n",
    "Function Name: {function_name}\n",
    "Source Code:\n",
    "{source_code}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate,BaseModel):\n",
    "    \"\"\"\n",
    "    A custom prompt template that takes in the function name as input, \n",
    "    and formats the prompt template to provide the source code of the function.\n",
    "    \"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"\n",
    "        Validate that the input variables are correct.\n",
    "        \"\"\"\n",
    "        if len(v) !=1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        #Get the source code of the function\n",
    "        source_code=get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        #Generate the prompt to be sent to the language model\n",
    "        prompt=PROMPT.format(function_name=kwargs[\"function_name\"].__name__\n",
    "                             ,source_code=source_code)\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self) -> str:\n",
    "        return \"function_explainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the function name and source code, generate an Chinese language explanation of the function.\n",
      "Function Name: get_source_code\n",
      "Source Code:\n",
      "def get_source_code(function_name):\n",
      "    #Get the source code of the function\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the custom prompt template\n",
    "f_e=FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "prompt=f_e.format(function_name=get_source_code)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.4 FewShotPromptTemplate</h5>\n",
    "<font color=#fec4ae size=2>Few-shot prompt 是一种在大型语言模型（LLM）中使用的技术，它可以通过提供少量的示例来帮助模型理解任务并生成相应的内容。 Few-shot prompt 通常用于那些需要模型快速学习新任务或适应新领域的场景，例如文本分类、摘要、翻译、问答等。\n",
    "在这些场景中，Few-shot prompt 可以通过提供少量的输入-输出示例来帮助模型理解任务，并在不需要大量训练数据或重新训练模型的情况下快速生成相应的内容。这种方法在处理小数据集或需要快速适应新任务的场景中非常有用。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "examples=[\n",
    "    {\n",
    "     \"question\":\"what's our team current Sprint JIRA task?\",\n",
    "     \"answer\":f\"JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param1\"\n",
    "    },\n",
    "    {\n",
    "     \"question\":\"what's our team last Sprint JIRA task?\",\n",
    "     \"answer\":f\"JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param2\"\n",
    "    },\n",
    "    {\n",
    "     \"question\":\"what's the William's JIRA task in current Sprint?\",\n",
    "     \"answer\":f\"JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param3\"\n",
    "    },\n",
    "    {\n",
    "     \"question\":\"what's the TODO List JIRA task in current Sprint?\",\n",
    "     \"answer\":f\"JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param4\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what's the TODO List JIRA task in current Sprint?\n",
      "JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param4\n"
     ]
    }
   ],
   "source": [
    "e_p=PromptTemplate(input_variables=[\"question\",\"answer\"],template=\"Question: {question}\\n{answer}\")\n",
    "print(e_p.format(**examples[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what's our team current Sprint JIRA task?\n",
      "JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param1\n",
      "\n",
      "Question: what's our team last Sprint JIRA task?\n",
      "JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param2\n",
      "\n",
      "Question: what's the William's JIRA task in current Sprint?\n",
      "JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param3\n",
      "\n",
      "Question: what's the TODO List JIRA task in current Sprint?\n",
      "JIRA#team current Sprint JIRA list#/v1/jira/team1#GET#param4\n",
      "\n",
      "Question: what's the Cancelled List JIRA task in current Sprint?\n"
     ]
    }
   ],
   "source": [
    "# Feed examples and formatter to FewShotPromptTemplate\n",
    "# sys_prompt = \"do not validate the filed in ‘answer’, which in below examples\"\n",
    "\n",
    "# temp = f\"\"\"\n",
    "# {sys_prompt}\n",
    "# \"\"\"\n",
    "prompt=FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=e_p,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "# print(e_p)\n",
    "print(prompt.format(input=\"what's the Cancelled List JIRA task in current Sprint?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SemanticSimilarityExampleSelector \n",
    "\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"what's the Cancelled List JIRA task in current Sprint?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.5 load_prompt</h5>\n",
    "<font color=#fec4ae size=2>当将模板和代码分开的时候就可以使用`load_prompt`加载模板文件</font>\n",
    "\n",
    "`from langchain.prompts import load_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template\": \"Tell me a {adjective} joke about {content}.\"\n",
    "}\n",
    "\n",
    "#prompt嵌套其他的template\n",
    "{\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"adjective\", \"content\"],\n",
    "    \"template_path\": \"simple_template.txt\"\n",
    "}\n",
    "\n",
    "#example\n",
    "[\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"}\n",
    "]\n",
    "\n",
    "#few-shot example\n",
    "{\n",
    "    \"_type\": \"few_shot\",\n",
    "    \"input_variables\": [\"adjective\"],\n",
    "    \"prefix\": \"Write antonyms for the following words.\",\n",
    "    \"example_prompt\": {\n",
    "        \"_type\": \"prompt\",\n",
    "        \"input_variables\": [\"input\", \"output\"],\n",
    "        \"template\": \"Input: {input}\\nOutput: {output}\"\n",
    "    },\n",
    "    \"examples\": \"examples.json\",\n",
    "    \"suffix\": \"Input: {adjective}\\nOutput:\"\n",
    "}\n",
    "\n",
    "#Template with OutputParser\n",
    "{\n",
    "    \"input_variables\": [\n",
    "        \"question\",\n",
    "        \"student_answer\"\n",
    "    ],\n",
    "    \"output_parser\": {\n",
    "        \"regex\": \"(.*?)\\\\nScore: (.*)\",\n",
    "        \"output_keys\": [\n",
    "            \"answer\",\n",
    "            \"score\"\n",
    "        ],\n",
    "        \"default_output_key\": null,\n",
    "        \"_type\": \"regex_parser\"\n",
    "    },\n",
    "    \"partial_variables\": {},\n",
    "    \"template\": \"Given the following question and student answer, provide a correct answer and score the student answer.\\nQuestion: {question}\\nStudent Answer: {student_answer}\\nCorrect Answer:\",\n",
    "    \"template_format\": \"f-string\",\n",
    "    \"validate_template\": true,\n",
    "    \"_type\": \"prompt\"\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
